{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import warnings\n",
    "\n",
    "# import aesara.tensor as at\n",
    "import pytensor.tensor as pt\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import scipy as sp\n",
    "\n",
    "from scipy.special import expit  # sigmoid\n",
    "from scipy import stats\n",
    "from scipy.special import expit as logistic\n",
    "from scipy.special import softmax\n",
    "from scipy.special import logit\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "RANDOM_SEED = 8927\n",
    "np.random.seed(286)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az.style.use(\"arviz-darkgrid\")\n",
    "az.rcParams[\"stats.hdi_prob\"] = 0.89\n",
    "\n",
    "\n",
    "def standardize(series):\n",
    "    \"\"\"Standardize a pandas series\"\"\"\n",
    "    return (series - series.mean()) / series.std()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.1"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "d = pd.read_csv(\"Data/chimpanzees.csv\", sep=\";\")\n",
    "# we change \"actor\" to zero-index\n",
    "d.actor = d.actor - 1\n",
    "d"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "d[\"treatment\"] = d.prosoc_left + 2 * d.condition\n",
    "d[[\"actor\", \"prosoc_left\", \"condition\", \"treatment\"]]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.3"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "d.groupby(\"treatment\").first()[[\"prosoc_left\", \"condition\"]]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.4 and 11.5"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with pm.Model() as m11_1:\n",
    "    a = pm.Normal(\"a\", 0.0, 10.0)\n",
    "    p = pm.Deterministic(\"p\", pm.math.invlogit(a))\n",
    "    pulled_left = pm.Binomial(\"pulled_left\", 1, p, observed=d.pulled_left)\n",
    "\n",
    "    prior_11_1 = pm.sample_prior_predictive(random_seed=RANDOM_SEED)\n",
    "idata_11_1 = prior_11_1[\"prior\"]\n",
    "\n",
    "# need it to recreate the prior pred plot:\n",
    "with pm.Model() as m11_1bis:\n",
    "    a = pm.Normal(\"a\", 0.0, 1.5)\n",
    "    p = pm.Deterministic(\"p\", pm.math.invlogit(a))\n",
    "    pulled_left = pm.Binomial(\"pulled_left\", 1, p, observed=d.pulled_left)\n",
    "\n",
    "    prior_11_1bis = pm.sample_prior_predictive(random_seed=RANDOM_SEED)\n",
    "idata_11_1bis = prior_11_1bis[\"prior\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.6"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ax = az.plot_density(\n",
    "    [idata_11_1, idata_11_1bis],\n",
    "    data_labels=[\"a ~ Normal(0, 10)\", \"a ~ Normal(0, 1.5)\"],\n",
    "    group=\"prior\",\n",
    "    colors=[\"k\", \"b\"],\n",
    "    var_names=[\"p\"],\n",
    "    point_estimate=None,\n",
    ")[0]\n",
    "ax[0].set_xlabel(\"prior prob pull left\")\n",
    "ax[0].set_ylabel(\"Density\")\n",
    "ax[0].set_title(\"Prior predictive simulations for p\");"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.7 - 11.9"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with pm.Model() as m11_2:\n",
    "    a = pm.Normal(\"a\", 0.0, 1.5)\n",
    "    b = pm.Normal(\"b\", 0.0, 10.0, shape=4)\n",
    "\n",
    "    p = pm.math.invlogit(a + b[d.treatment.values])\n",
    "    pulled_left = pm.Binomial(\"pulled_left\", 1, p, observed=d.pulled_left)\n",
    "\n",
    "    prior_11_2 = pm.sample_prior_predictive(random_seed=RANDOM_SEED)\n",
    "prior_2 = prior_11_2[\"prior\"]\n",
    "\n",
    "with pm.Model() as m11_3:\n",
    "    a = pm.Normal(\"a\", 0.0, 1.5)\n",
    "    b = pm.Normal(\"b\", 0.0, 0.5, shape=4)\n",
    "\n",
    "    p = pm.math.invlogit(a + b[d.treatment.values])\n",
    "    pulled_left = pm.Binomial(\"pulled_left\", 1, p, observed=d.pulled_left)\n",
    "\n",
    "    prior_11_3 = pm.sample_prior_predictive(random_seed=RANDOM_SEED)\n",
    "prior_3 = prior_11_3[\"prior\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "p_treat1, p_treat2 = (\n",
    "    logistic(prior_2[\"a\"] + prior_2[\"b\"].sel(b_dim_0=0)),\n",
    "    logistic(prior_2[\"a\"] + prior_2[\"b\"].sel(b_dim_0=1)),\n",
    ")\n",
    "p_treat1_bis, p_treat2_bis = (\n",
    "    logistic(prior_3[\"a\"] + prior_3[\"b\"].sel(b_dim_0=0)),\n",
    "    logistic(prior_3[\"a\"] + prior_3[\"b\"].sel(b_dim_0=1)),\n",
    ")\n",
    "\n",
    "ax = az.plot_density(\n",
    "    [np.abs(p_treat1 - p_treat2).values, np.abs(p_treat1_bis - p_treat2_bis).values],\n",
    "    data_labels=[\"b ~ Normal(0, 10)\", \"b ~ Normal(0, 0.5)\"],\n",
    "    group=\"prior\",\n",
    "    colors=[\"k\", \"b\"],\n",
    "    point_estimate=None,\n",
    ")[0]\n",
    "ax[0].set_xlabel(\"prior diff between treatments\")\n",
    "ax[0].set_ylabel(\"Density\")\n",
    "ax[0].set_title(None);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "np.abs(p_treat1_bis - p_treat2_bis).mean().values"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# p_treat1_bis"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.10"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "actor_idx, actors = pd.factorize(d.actor)\n",
    "treat_idx, treatments = pd.factorize(d.treatment)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.11"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with pm.Model() as m11_4:\n",
    "    a = pm.Normal(\"a\", 0.0, 1.5, shape=len(actors))\n",
    "    b = pm.Normal(\"b\", 0.0, 0.5, shape=len(treatments))\n",
    "\n",
    "    actor_id = pm.intX(pm.Data(\"actor_id\", actor_idx))\n",
    "    treat_id = pm.intX(pm.Data(\"treat_id\", treat_idx))\n",
    "    p = pm.Deterministic(\"p\", pm.math.invlogit(a[actor_id] + b[treat_id]))\n",
    "\n",
    "    pulled_left = pm.Binomial(\"pulled_left\", 1, p, observed=d.pulled_left)\n",
    "\n",
    "    trace_11_4 = pm.sample(random_seed=RANDOM_SEED)\n",
    "\n",
    "az.summary(trace_11_4, var_names=[\"a\", \"b\"], round_to=2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.12"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az.plot_forest(trace_11_4, var_names=[\"a\"], transform=logistic, combined=True);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.13"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ax = az.plot_forest(trace_11_4, var_names=[\"b\"], combined=True)\n",
    "ax[0].set_yticklabels([\"L/P\", \"R/P\", \"L/N\", \"R/N\"]);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.14"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "db13 = trace_11_4.posterior[\"b\"].sel(b_dim_0=0) - trace_11_4.posterior[\"b\"].sel(b_dim_0=2)\n",
    "db24 = trace_11_4.posterior[\"b\"].sel(b_dim_0=1) - trace_11_4.posterior[\"b\"].sel(b_dim_0=3)\n",
    "az.plot_forest([db13.values, db24.values], model_names=[\"db13\", \"db24\"], combined=True);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.15"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pl = d.groupby([\"actor\", \"treatment\"]).agg(\"mean\")[\"pulled_left\"].unstack()\n",
    "pl"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.16 and 11.17"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with m11_4:\n",
    "    pm.set_data({\"actor_id\": np.repeat(range(7), 4), \"treat_id\": list(range(4)) * 7})\n",
    "    ppd = pm.sample_posterior_predictive(trace_11_4, random_seed=RANDOM_SEED, var_names=[\"p\"])[\n",
    "        \"posterior_predictive\"\n",
    "    ][\"p\"]\n",
    "p_mu = np.array(ppd.mean([\"chain\", \"draw\"])).reshape((7, 4))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pl_val = pl.stack().values\n",
    "_, (ax0, ax1) = plt.subplots(2, 1, figsize=(12, 6))\n",
    "alpha, xoff, yoff = 0.6, 0.3, 0.05\n",
    "\n",
    "ax0.plot([7 * 4 - 0.5] * 2, [0, 1], c=\"k\", alpha=0.4, lw=1)\n",
    "ax0.axhline(0.5, ls=\"--\", c=\"k\", alpha=0.4)\n",
    "for actor in range(len(actors)):\n",
    "    ax0.plot(\n",
    "        [actor * 4, actor * 4 + 2],\n",
    "        [pl.loc[actor, 0], pl.loc[actor, 2]],\n",
    "        \"-\",\n",
    "        c=\"b\",\n",
    "        alpha=alpha,\n",
    "    )\n",
    "    ax0.plot(\n",
    "        [actor * 4 + 1, actor * 4 + 3],\n",
    "        [pl.loc[actor, 1], pl.loc[actor, 3]],\n",
    "        \"-\",\n",
    "        c=\"b\",\n",
    "        alpha=alpha,\n",
    "    )\n",
    "    ax0.plot(\n",
    "        [actor * 4, actor * 4 + 1],\n",
    "        [pl.loc[actor, 0], pl.loc[actor, 1]],\n",
    "        \"o\",\n",
    "        c=\"b\",\n",
    "        fillstyle=\"none\",\n",
    "        ms=6,\n",
    "        alpha=alpha,\n",
    "    )\n",
    "    ax0.plot(\n",
    "        [actor * 4 + 2, actor * 4 + 3],\n",
    "        [pl.loc[actor, 2], pl.loc[actor, 3]],\n",
    "        \"o\",\n",
    "        c=\"b\",\n",
    "        ms=6,\n",
    "        alpha=alpha,\n",
    "    )\n",
    "    ax0.plot([actor * 4 - 0.5] * 2, [0, 1], c=\"k\", alpha=0.4, lw=1)\n",
    "    ax0.text(actor * 4 + 0.5, 1.1, f\"actor {actor + 1}\", fontsize=12)\n",
    "    if actor == 0:\n",
    "        ax0.text(actor * 4 - xoff, pl.loc[actor, 0] - 2 * yoff, \"R/N\")\n",
    "        ax0.text(actor * 4 + 1 - xoff, pl.loc[actor, 1] + yoff, \"L/N\")\n",
    "        ax0.text(actor * 4 + 2 - xoff, pl.loc[actor, 2] - 2 * yoff, \"R/P\")\n",
    "        ax0.text(actor * 4 + 3 - xoff, pl.loc[actor, 3] + yoff, \"L/P\")\n",
    "ax0.set_xticks([])\n",
    "ax0.set_ylabel(\"proportion left lever\", labelpad=10)\n",
    "ax0.set_title(\"observed proportions\", pad=25)\n",
    "\n",
    "ax1.plot([range(28), range(28)], az.hdi(ppd)[\"p\"].T, \"k-\", lw=2, alpha=alpha)\n",
    "ax1.plot([7 * 4 - 0.5] * 2, [0, 1], c=\"k\", alpha=0.4, lw=1)\n",
    "ax1.axhline(0.5, ls=\"--\", c=\"k\", alpha=0.4)\n",
    "for actor in range(len(actors)):\n",
    "    ax1.plot(\n",
    "        [actor * 4, actor * 4 + 2],\n",
    "        [p_mu[actor, 0], p_mu[actor, 2]],\n",
    "        \"-\",\n",
    "        c=\"k\",\n",
    "        alpha=alpha,\n",
    "    )\n",
    "    ax1.plot(\n",
    "        [actor * 4 + 1, actor * 4 + 3],\n",
    "        [p_mu[actor, 1], p_mu[actor, 3]],\n",
    "        \"-\",\n",
    "        c=\"k\",\n",
    "        alpha=alpha,\n",
    "    )\n",
    "    ax1.plot(\n",
    "        [actor * 4, actor * 4 + 1],\n",
    "        [p_mu[actor, 0], p_mu[actor, 1]],\n",
    "        \"o\",\n",
    "        c=\"k\",\n",
    "        fillstyle=\"none\",\n",
    "        ms=6,\n",
    "        alpha=alpha,\n",
    "    )\n",
    "    ax1.plot(\n",
    "        [actor * 4 + 2, actor * 4 + 3],\n",
    "        [p_mu[actor, 2], p_mu[actor, 3]],\n",
    "        \"o\",\n",
    "        c=\"k\",\n",
    "        ms=6,\n",
    "        alpha=alpha,\n",
    "    )\n",
    "    ax1.plot([actor * 4 - 0.5] * 2, [0, 1], c=\"k\", alpha=0.4, lw=1)\n",
    "    ax1.text(actor * 4 + 0.5, 1.1, f\"actor {actor + 1}\", fontsize=12)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_ylabel(\"proportion left lever\", labelpad=10)\n",
    "ax1.set_title(\"posterior predictions\", pad=25)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.18"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "side = d.prosoc_left.values  # right 0, left 1\n",
    "cond = d.condition.values  # no partner 0, partner 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.19"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with pm.Model() as m11_5:\n",
    "    a = pm.Normal(\"a\", 0.0, 1.5, shape=len(actors))\n",
    "    bs = pm.Normal(\"bs\", 0.0, 0.5, shape=2)\n",
    "    bc = pm.Normal(\"bc\", 0.0, 0.5, shape=2)\n",
    "\n",
    "    p = pm.math.invlogit(a[actor_idx] + bs[side] + bc[cond])\n",
    "\n",
    "    pulled_left = pm.Binomial(\"pulled_left\", 1, p, observed=d.pulled_left)\n",
    "\n",
    "    trace_11_5 = pm.sample(random_seed=RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.20\n",
    "As we changed the data of `m11_4` above, we need to sample from it again, with the original data:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with m11_4:\n",
    "    pm.set_data({\"actor_id\": actor_idx, \"treat_id\": treat_idx})\n",
    "    trace_11_4 = pm.sample(random_seed=RANDOM_SEED)\n",
    "\n",
    "az.compare({\"m11_4\": trace_11_4, \"m11_5\": trace_11_5})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.23"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "post_b_11_4 = az.extract_dataset(trace_11_4[\"posterior\"])[\"b\"]\n",
    "np.exp(np.array(post_b_11_4[3, :]) - np.array(post_b_11_4[1, :])).mean().round(3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.24"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "d = pd.read_csv(\"Data/chimpanzees.csv\", sep=\";\")\n",
    "d.actor = d.actor - 1\n",
    "d[\"treatment\"] = d.prosoc_left + 2 * d.condition\n",
    "\n",
    "d_aggregated = (\n",
    "    d.groupby([\"treatment\", \"actor\"]).sum().reset_index()[[\"treatment\", \"actor\", \"pulled_left\"]]\n",
    ")\n",
    "d_aggregated.head(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.25"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with pm.Model() as m11_6:\n",
    "    a = pm.Normal(\"a\", 0.0, 1.5, shape=len(actors))\n",
    "    b = pm.Normal(\"b\", 0.0, 0.5, shape=len(treatments))\n",
    "\n",
    "    p = pm.Deterministic(\n",
    "        \"p\", pm.math.invlogit(a[d_aggregated.actor.values] + b[d_aggregated.treatment.values])\n",
    "    )\n",
    "\n",
    "    pulled_left = pm.Binomial(\"pulled_left\", 18, p, observed=d_aggregated.pulled_left)\n",
    "\n",
    "    trace_11_6 = pm.sample(random_seed=RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.26\n",
    "ArviZ won't even let you compare models with different observations:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az.compare({\"m11_4\": trace_11_4, \"m11_6\": trace_11_6})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.27"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# deviance of aggregated 6-in-9\n",
    "(-2 * stats.binom.logpmf(6, 9, 0.2)).round(5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# deviance of dis-aggregated\n",
    "-2 * stats.bernoulli.logpmf([1, 1, 1, 1, 1, 1, 0, 0, 0], 0.2).sum().round(5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.28"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "d_ad = pd.read_csv(\"Data/UCBadmit.csv\", sep=\";\")\n",
    "d_ad"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.29"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "gid = (d_ad[\"applicant.gender\"] == \"female\").astype(int).values\n",
    "\n",
    "with pm.Model() as m11_7:\n",
    "    a = pm.Normal(\"a\", 0, 1.5, shape=2)\n",
    "    p = pm.Deterministic(\"p\", pm.math.invlogit(a[gid]))\n",
    "\n",
    "    admit = pm.Binomial(\"admit\", p=p, n=d_ad.applications.values, observed=d_ad.admit.values)\n",
    "\n",
    "    trace_11_7 = pm.sample(random_seed=RANDOM_SEED)\n",
    "az.summary(trace_11_7, var_names=[\"a\"], round_to=2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.30"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "post_a_11_7 = az.extract_dataset(trace_11_7[\"posterior\"])[\"a\"]\n",
    "diff_a = post_a_11_7[0, :] - post_a_11_7[1, :]\n",
    "diff_p = logistic(post_a_11_7[0, :]) - logistic(post_a_11_7[1, :])\n",
    "az.summary({\"diff_a\": diff_a, \"diff_p\": diff_p}, kind=\"stats\", round_to=2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.31"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with m11_7:\n",
    "    ppc = pm.sample_posterior_predictive(\n",
    "        trace_11_7, random_seed=RANDOM_SEED, var_names=[\"admit\", \"p\"]\n",
    "    )\n",
    "    pp_p = ppc[\"posterior_predictive\"][\"p\"]\n",
    "    pp_admit = ppc[\"posterior_predictive\"][\"admit\"] / d_ad.applications.values[None, :]\n",
    "\n",
    "\n",
    "p_mu = np.array(pp_p.mean([\"chain\", \"draw\"]))\n",
    "p_std = np.array(pp_p.std([\"chain\", \"draw\"]))\n",
    "admit_mu = np.array(pp_admit.mean([\"chain\", \"draw\"]))\n",
    "admit_std = np.array(pp_admit.std([\"chain\", \"draw\"]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for i in range(6):\n",
    "    x = 1 + 2 * i\n",
    "\n",
    "    y1 = d_ad.admit[x] / d_ad.applications[x]\n",
    "    y2 = d_ad.admit[x + 1] / d_ad.applications[x + 1]\n",
    "\n",
    "    plt.plot([x, x + 1], [y1, y2], \"-C0o\", alpha=0.6, lw=2)\n",
    "    plt.text(x + 0.25, (y1 + y2) / 2 + 0.05, d_ad.dept[x])\n",
    "\n",
    "plt.plot(range(1, 13), p_mu, \"ko\", fillstyle=\"none\", ms=6, alpha=0.6)\n",
    "plt.plot([range(1, 13), range(1, 13)], az.hdi(trace_11_7)[\"p\"].T, \"k-\", lw=1, alpha=0.6)\n",
    "plt.plot([range(1, 13), range(1, 13)], az.hdi(pp_admit)[\"admit\"].T, \"k+\", ms=6, alpha=0.6)\n",
    "\n",
    "plt.xlabel(\"case\")\n",
    "plt.ylabel(\"admit\")\n",
    "plt.title(\"Posterior validation check\")\n",
    "plt.ylim(0, 1);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.32"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dept_id = pd.Categorical(d_ad[\"dept\"]).codes\n",
    "\n",
    "with pm.Model() as m11_8:\n",
    "    a = pm.Normal(\"a\", 0, 1.5, shape=2)\n",
    "    delta = pm.Normal(\"delta\", 0, 1.5, shape=6)\n",
    "\n",
    "    p = pm.math.invlogit(a[gid] + delta[dept_id])\n",
    "\n",
    "    admit = pm.Binomial(\"admit\", p=p, n=d_ad.applications.values, observed=d_ad.admit.values)\n",
    "\n",
    "    trace_11_8 = pm.sample(2000, random_seed=RANDOM_SEED)\n",
    "az.summary(trace_11_8, round_to=2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.33"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "post_a_11_8 = az.extract_dataset(trace_11_8[\"posterior\"])[\"a\"]\n",
    "diff_a = post_a_11_8[0, :] - post_a_11_8[1, :]\n",
    "diff_p = logistic(post_a_11_8[0, :]) - logistic(post_a_11_8[1, :])\n",
    "az.summary({\"diff_a\": diff_a, \"diff_p\": diff_p}, kind=\"stats\", round_to=2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.34"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pg = pd.DataFrame(index=[\"male\", \"female\"], columns=d_ad.dept.unique())\n",
    "for dep in pg.columns:\n",
    "    pg[dep] = (\n",
    "        d_ad.loc[d_ad.dept == dep, \"applications\"]\n",
    "        / d_ad.loc[d_ad.dept == dep, \"applications\"].sum()\n",
    "    ).values\n",
    "pg.round(2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.35"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "y = np.random.binomial(n=1000, p=1 / 1000, size=10_000)\n",
    "y.mean(), y.var()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.36"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dk = pd.read_csv(\"Data/Kline\", sep=\";\")\n",
    "dk"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.37"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "P = standardize(np.log(dk.population)).values\n",
    "c_id = (dk.contact == \"high\").astype(int).values"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.38"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ax = az.plot_kde(\n",
    "    np.random.lognormal(0.0, 10.0, size=10_000),\n",
    "    label=\"a ~ LogNormal(0, 10)\",\n",
    "    plot_kwargs={\"color\": \"k\"},\n",
    ")\n",
    "ax.set_xlabel(\"mean number of tools\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_title(\"\");"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.39"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "a = np.random.normal(0.0, 10.0, size=10_000)\n",
    "np.exp(a).mean()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.40"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ax = az.plot_kde(np.random.lognormal(3.0, 0.5, size=20_000), label=\"a ~ LogNormal(3, 0.5)\")\n",
    "ax.set_xlabel(\"mean number of tools\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_title(\"\");"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.41 to 11.44"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def kline_prior_plot(N: int = 100, b_prior: str = \"bespoke\", x_scale: str = \"stdz\", ax=None):\n",
    "    \"\"\"\n",
    "    Utility function to plot prior predictive checks for Kline Poisson model.\n",
    "    N: number of prior predictive trends.\n",
    "\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.set_ylabel(\"total tools\")\n",
    "\n",
    "    itcpts = np.random.normal(3.0, 0.5, N)\n",
    "    if b_prior == \"conventional\":\n",
    "        slopes = np.random.normal(0.0, 10.0, N)\n",
    "        ax.set_title(\"b ~ Normal(0, 10)\")\n",
    "    elif b_prior == \"bespoke\":\n",
    "        slopes = np.random.normal(0.0, 0.2, N)\n",
    "        ax.set_title(\"b ~ Normal(0, 0.2)\")\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Prior for slopes (b_prior) can only be either 'conventional' or 'bespoke'.\"\n",
    "        )\n",
    "\n",
    "    x_seq = np.linspace(np.log(100), np.log(200_000), N)\n",
    "    ax.set_ylim((0, 500))\n",
    "    if x_scale == \"log\":\n",
    "        for a, b in zip(itcpts, slopes):\n",
    "            ax.plot(x_seq, np.exp(a + b * x_seq), \"k\", alpha=0.4)\n",
    "        ax.set_xlabel(\"log population\")\n",
    "    elif x_scale == \"natural\":\n",
    "        for a, b in zip(itcpts, slopes):\n",
    "            ax.plot(np.exp(x_seq), np.exp(a + b * x_seq), \"k\", alpha=0.4)\n",
    "        ax.set_xlabel(\"population\")\n",
    "    else:\n",
    "        x_seq = np.linspace(-2, 2, N)\n",
    "        for a, b in zip(itcpts, slopes):\n",
    "            ax.plot(x_seq, np.exp(a + b * x_seq), \"k\", alpha=0.4)\n",
    "        ax.set_ylim((0, 100))\n",
    "        ax.set_xlabel(\"log population (std)\")\n",
    "\n",
    "    return ax"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "_, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "kline_prior_plot(b_prior=\"conventional\", x_scale=\"stdz\", ax=ax[0][0])\n",
    "kline_prior_plot(b_prior=\"bespoke\", x_scale=\"stdz\", ax=ax[0][1])\n",
    "kline_prior_plot(b_prior=\"bespoke\", x_scale=\"log\", ax=ax[1][0])\n",
    "kline_prior_plot(x_scale=\"natural\", ax=ax[1][1])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.45"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# intercept only\n",
    "with pm.Model() as m11_9:\n",
    "    a = pm.Normal(\"a\", 3.0, 0.5)\n",
    "    T = pm.Poisson(\"total_tools\", pm.math.exp(a), observed=dk.total_tools)\n",
    "    trace_11_9 = pm.sample(tune=3000, random_seed=RANDOM_SEED)\n",
    "\n",
    "# interaction model\n",
    "with pm.Model() as m11_10:\n",
    "    a = pm.Normal(\"a\", 3.0, 0.5, shape=2)\n",
    "    b = pm.Normal(\"b\", 0.0, 0.2, shape=2)\n",
    "\n",
    "    cid = pm.intX(pm.Data(\"cid\", c_id))\n",
    "    P_ = pm.Data(\"P\", P)\n",
    "    lam = pm.math.exp(a[cid] + b[cid] * P_)\n",
    "\n",
    "    T = pm.Poisson(\"total_tools\", lam, observed=dk.total_tools)\n",
    "    trace_11_10 = pm.sample(tune=3000, random_seed=RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.46"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az.compare({\"m11_9\": trace_11_9, \"m11_10\": trace_11_10})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# store pareto-k values for plot:\n",
    "k = az.loo(trace_11_10, pointwise=True).pareto_k.values"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.47 and 11.48"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ns = 100\n",
    "P_seq = np.linspace(-1.4, 3.0, ns)\n",
    "\n",
    "with m11_10:\n",
    "    # predictions for cid=0 (low contact)\n",
    "    pm.set_data({\"cid\": np.array([0] * ns), \"P\": P_seq})\n",
    "    lam0 = pm.sample_posterior_predictive(trace_11_10, var_names=[\"total_tools\"])[\n",
    "        \"posterior_predictive\"\n",
    "    ][\"total_tools\"]\n",
    "\n",
    "    # predictions for cid=1 (high contact)\n",
    "    pm.set_data({\"cid\": np.array([1] * ns)})\n",
    "    lam1 = pm.sample_posterior_predictive(trace_11_10, var_names=[\"total_tools\"])[\n",
    "        \"posterior_predictive\"\n",
    "    ][\"total_tools\"]\n",
    "\n",
    "lmu0, lmu1 = lam0.mean([\"chain\", \"draw\"]), lam1.mean([\"chain\", \"draw\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "_, (ax0, ax1) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# scale point size to Pareto-k:\n",
    "k /= k.max()\n",
    "psize = 250 * k\n",
    "\n",
    "# Plot on standardized log scale:\n",
    "\n",
    "az.plot_hdi(P_seq, lam1, color=\"b\", fill_kwargs={\"alpha\": 0.2}, ax=ax0)\n",
    "ax0.plot(P_seq, lmu1, color=\"b\", alpha=0.7, label=\"high contact mean\")\n",
    "\n",
    "az.plot_hdi(P_seq, lam0, color=\"k\", fill_kwargs={\"alpha\": 0.2}, ax=ax0)\n",
    "ax0.plot(P_seq, lmu0, \"--\", color=\"k\", alpha=0.7, label=\"low contact mean\")\n",
    "\n",
    "# display names and k:\n",
    "mask = k > 0.3\n",
    "labels = dk.culture.values[mask]\n",
    "for i, text in enumerate(labels):\n",
    "    ax0.text(\n",
    "        P[mask][i] - 0.2,\n",
    "        dk.total_tools.values[mask][i] + 4,\n",
    "        f\"{text} ({np.round(k[mask][i], 2)})\",\n",
    "        fontsize=8,\n",
    "    )\n",
    "\n",
    "# display observed data:\n",
    "index = c_id == 1\n",
    "ax0.scatter(\n",
    "    P[~index],\n",
    "    dk.total_tools[~index],\n",
    "    s=psize[~index],\n",
    "    facecolors=\"none\",\n",
    "    edgecolors=\"k\",\n",
    "    alpha=0.8,\n",
    "    lw=1,\n",
    "    label=\"low contact\",\n",
    ")\n",
    "ax0.scatter(P[index], dk.total_tools[index], s=psize[index], alpha=0.8, label=\"high contact\")\n",
    "ax0.set_xlabel(\"log population (std)\")\n",
    "ax0.set_ylabel(\"total tools\")\n",
    "ax0.legend(fontsize=8, ncol=2)\n",
    "\n",
    "# Plot on natural scale:\n",
    "# unstandardize and exponentiate values of standardized log pop:\n",
    "P_seq = np.linspace(-5.0, 3.0, ns)\n",
    "P_seq = np.exp(P_seq * np.log(dk.population.values).std() + np.log(dk.population.values).mean())\n",
    "\n",
    "az.plot_hdi(P_seq, lam1, color=\"b\", fill_kwargs={\"alpha\": 0.2}, ax=ax1)\n",
    "ax1.plot(P_seq, lmu1, color=\"b\", alpha=0.7)\n",
    "\n",
    "az.plot_hdi(P_seq, lam0, color=\"k\", fill_kwargs={\"alpha\": 0.2}, ax=ax1)\n",
    "ax1.plot(P_seq, lmu0, \"--\", color=\"k\", alpha=0.7)\n",
    "\n",
    "# display observed data:\n",
    "ax1.scatter(\n",
    "    dk.population[~index],\n",
    "    dk.total_tools[~index],\n",
    "    s=psize[~index],\n",
    "    facecolors=\"none\",\n",
    "    edgecolors=\"k\",\n",
    "    alpha=0.8,\n",
    "    lw=1,\n",
    ")\n",
    "ax1.scatter(dk.population[index], dk.total_tools[index], s=psize[index], alpha=0.8)\n",
    "plt.setp(ax1.get_xticklabels(), ha=\"right\", rotation=45)\n",
    "ax1.set_xlim((-10_000, 350_000))\n",
    "ax1.set_xlabel(\"population\")\n",
    "ax1.set_ylabel(\"total tools\");"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.49\n",
    "The book doesn't pre-process the population data, but if you give them raw to PyMC, the sampler will break: the scale of these data is too wide. However we can't just standardize the data, as we usually do. Why? Because some data points will then be negative, which doesn't play nice with the `b` exponent (try it if you don't trust me). But we'll do something similar: let's standardize the data, and then just add the absolute value of the minimum, and add yet again an epsilon -- this will ensure that our data stay positive and that the transformation will be easy to reverse when we want to plot on the natural scale:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "P = standardize(np.log(dk.population)).values\n",
    "P = P + np.abs(P.min()) + 0.1  # must be > 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can run the model:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with pm.Model() as m11_11:\n",
    "    a = pm.Normal(\"a\", 1.0, 1.0, shape=2)\n",
    "    b = pm.Exponential(\"b\", 1.0, shape=2)\n",
    "    g = pm.Exponential(\"g\", 1.0)\n",
    "\n",
    "    cid = pm.intX(pm.Data(\"cid\", c_id))\n",
    "    P_ = pm.Data(\"P\", P)\n",
    "    lam = (at.exp(a[cid]) * P_ ** b[cid]) / g\n",
    "\n",
    "    T = pm.Poisson(\"total_tools\", lam, observed=dk.total_tools.values)\n",
    "    trace_11_11 = pm.sample(2000, tune=2000, target_accept=0.9, random_seed=RANDOM_SEED)\n",
    "az.plot_trace(trace_11_11, compact=True);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus: posterior predictive plot for scientific model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ns = 100\n",
    "P_seq = np.linspace(-1.4, 3.0, ns) + 1.4  # our little trick\n",
    "\n",
    "with m11_11:\n",
    "    # predictions for cid=0 (low contact)\n",
    "    pm.set_data({\"cid\": np.array([0] * ns), \"P\": P_seq})\n",
    "    lam0 = pm.sample_posterior_predictive(trace_11_11, var_names=[\"total_tools\"])[\n",
    "        \"posterior_predictive\"\n",
    "    ][\"total_tools\"]\n",
    "\n",
    "    # predictions for cid=1 (high contact)\n",
    "    pm.set_data({\"cid\": np.array([1] * ns)})\n",
    "    lam1 = pm.sample_posterior_predictive(trace_11_11, var_names=[\"total_tools\"])[\n",
    "        \"posterior_predictive\"\n",
    "    ][\"total_tools\"]\n",
    "\n",
    "lmu0, lmu1 = lam0.mean([\"chain\", \"draw\"]), lam1.mean([\"chain\", \"draw\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "_, (ax0, ax1) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot on standardized log scale:\n",
    "az.plot_hdi(P_seq, lam1, color=\"b\", fill_kwargs={\"alpha\": 0.2}, ax=ax0)\n",
    "ax0.plot(P_seq, lmu1, color=\"b\", alpha=0.7, label=\"high contact mean\")\n",
    "\n",
    "az.plot_hdi(P_seq, lam0, color=\"k\", fill_kwargs={\"alpha\": 0.2}, ax=ax0)\n",
    "ax0.plot(P_seq, lmu0, \"--\", color=\"k\", alpha=0.7, label=\"low contact mean\")\n",
    "\n",
    "# display observed data:\n",
    "index = c_id == 1\n",
    "ax0.scatter(\n",
    "    P[~index],\n",
    "    dk.total_tools[~index],\n",
    "    s=psize[~index],\n",
    "    facecolors=\"none\",\n",
    "    edgecolors=\"k\",\n",
    "    alpha=0.8,\n",
    "    lw=1,\n",
    "    label=\"low contact\",\n",
    ")\n",
    "ax0.scatter(P[index], dk.total_tools[index], s=psize[index], alpha=0.8, label=\"high contact\")\n",
    "ax0.set_xlabel(\"standardized population\")\n",
    "ax0.set_ylabel(\"total tools\")\n",
    "ax0.legend(fontsize=8, ncol=2)\n",
    "\n",
    "# Plot on natural scale:\n",
    "# unstandardize our log pop sequence:\n",
    "P_seq = np.exp(\n",
    "    (P_seq - 1.4) * np.log(dk.population.values).std() + np.log(dk.population.values).mean()\n",
    ")\n",
    "\n",
    "az.plot_hdi(P_seq, lam1, color=\"b\", fill_kwargs={\"alpha\": 0.2}, ax=ax1)\n",
    "ax1.plot(P_seq, lmu1, color=\"b\", alpha=0.7)\n",
    "\n",
    "az.plot_hdi(P_seq, lam0, color=\"k\", fill_kwargs={\"alpha\": 0.2}, ax=ax1)\n",
    "ax1.plot(P_seq, lmu0, \"--\", color=\"k\", alpha=0.7)\n",
    "\n",
    "# display observed data:\n",
    "ax1.scatter(\n",
    "    dk.population[~index],\n",
    "    dk.total_tools[~index],\n",
    "    s=psize[~index],\n",
    "    facecolors=\"none\",\n",
    "    edgecolors=\"k\",\n",
    "    alpha=0.8,\n",
    "    lw=1,\n",
    ")\n",
    "ax1.scatter(dk.population[index], dk.total_tools[index], s=psize[index], alpha=0.8)\n",
    "plt.setp(ax1.get_xticklabels(), ha=\"right\", rotation=45)\n",
    "ax1.set_xlim((-10_000, 350_000))\n",
    "ax1.set_xlabel(\"population\")\n",
    "ax1.set_ylabel(\"total tools\");"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.50"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "num_days = 30\n",
    "y = np.random.poisson(1.5, num_days)\n",
    "y"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.51"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "num_weeks = 4\n",
    "y_new = np.random.poisson(0.5 * 7, num_weeks)\n",
    "y_new"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.52"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "y_all = np.hstack([y, y_new])\n",
    "exposure = np.hstack([np.repeat(1, 30), np.repeat(7, 4)]).astype(\"float\")\n",
    "monastery = np.hstack([np.repeat(0, 30), np.repeat(1, 4)])\n",
    "d = pd.DataFrame({\"y\": y_all, \"days\": exposure, \"monastery\": monastery})\n",
    "d"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.53"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# compute the offset:\n",
    "log_days = np.log(exposure)\n",
    "\n",
    "# fit the model:\n",
    "with pm.Model() as m11_12:\n",
    "    a = pm.Normal(\"a\", 0.0, 1.0)\n",
    "    b = pm.Normal(\"b\", 0.0, 1.0)\n",
    "\n",
    "    lam = pm.math.exp(log_days + a + b * monastery)\n",
    "\n",
    "    obs = pm.Poisson(\"y\", lam, observed=y_all)\n",
    "\n",
    "    trace_11_12 = pm.sample(1000, tune=2000, random_seed=RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.54"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "lambda_old = np.exp(trace_11_12[\"posterior\"][\"a\"])\n",
    "lambda_new = np.exp(trace_11_12[\"posterior\"][\"a\"] + trace_11_12[\"posterior\"][\"b\"])\n",
    "\n",
    "az.summary({\"lambda_old\": lambda_old, \"lambda_new\": lambda_new}, kind=\"stats\", round_to=2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.55"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# simulate career choices among 500 individuals\n",
    "N = 500  # number of individuals\n",
    "income = np.array([1, 2, 5])  # expected income of each career\n",
    "score = 0.5 * income  # score for each career, based on income\n",
    "# converts scores to probabilities:\n",
    "p = softmax(score)\n",
    "\n",
    "# now simulate choice\n",
    "# outcome career holds event type values, not counts\n",
    "career = np.random.multinomial(1, p, size=N)\n",
    "career = np.where(career == 1)[1]\n",
    "career[:11], score, p"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.56 and 11.57\n",
    "\n",
    "The model described in the book does not sample well in PyMC. It does slightly better if we change the pivot category to be the first career instead of the third, but this is still suboptimal because we are discarding predictive information from the pivoted category (i.e., its unique career income). \n",
    "\n",
    "In fact, it is not necessary to pivot the coefficients of variables that are distinct for each category (what the author calls predictors matched to outcomes), as it is done for the coefficients of shared variables (what the author calles predictors matched to observations). The intercepts belong to the second category, and as such they still need to be pivoted. These two references explain this distinction clearly: \n",
    "\n",
    "* Hoffman, S. D., & Duncan, G. J. (1988). Multinomial and conditional logit discrete-choice models in demography. Demography, 25(3), 415-427 [pdf link](https://www.jstor.org/stable/pdf/2061541.pdf)\n",
    "* Croissant, Y. (2020). Estimation of Random Utility Models in R: The mlogit Package. Journal of Statistical Software, 95(1), 1-41 [pdf link](https://www.jstatsoft.org/index.php/jss/article/view/v095i11/v95i11.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with pm.Model() as m11_13:\n",
    "    a = pm.Normal(\"a\", 0.0, 1.0, shape=2)  # intercepts\n",
    "    b = pm.HalfNormal(\"b\", 0.5)  # association of income with choice\n",
    "\n",
    "    s0 = a[0] + b * income[0]\n",
    "    s1 = a[1] + b * income[1]\n",
    "    s2 = 0.0 + b * income[2]  # pivoting the intercept for the third category\n",
    "    s = pm.math.stack([s0, s1, s2])\n",
    "\n",
    "    p_ = at.nnet.softmax(s)\n",
    "    career_obs = pm.Categorical(\"career\", p=p_, observed=career)\n",
    "\n",
    "    trace_11_13 = pm.sample(tune=2000, target_accept=0.99, random_seed=RANDOM_SEED)\n",
    "az.summary(trace_11_13, round_to=2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.58\n",
    "\n",
    "Because this model better matches the actual data generating process, it also does a better job at predicting the effect of doubling the income of the second career."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# set up logit scores:\n",
    "post_11_13 = az.extract_dataset(trace_11_13[\"posterior\"])\n",
    "s0 = post_11_13[\"a\"][0, :] + post_11_13[\"b\"] * income[0]\n",
    "s1_orig = post_11_13[\"a\"][1, :] + post_11_13[\"b\"] * income[1]\n",
    "s1_new = post_11_13[\"a\"][1, :] + post_11_13[\"b\"] * income[1] * 2\n",
    "s2 = 0.0 + post_11_13[\"b\"] * income[2]\n",
    "\n",
    "pp_scores_orig = np.stack([np.array(s0), np.array(s1_orig), np.array(s2)]).T\n",
    "pp_scores_new = np.stack([np.array(s0), np.array(s1_new), s2]).T\n",
    "\n",
    "# compute probabilities for original and counterfactual:\n",
    "p_orig = softmax(pp_scores_orig, axis=1)\n",
    "print(p_orig.shape)\n",
    "p_new = softmax(pp_scores_new, axis=1)\n",
    "print(p_new.shape)\n",
    "\n",
    "# summarize\n",
    "p_diff = p_new[:, 1] - p_orig[:, 1]\n",
    "az.summary({\"p_diff\": p_diff}, kind=\"stats\", round_to=2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Actual difference when doubling the income of option #2\n",
    "income_orig = np.array((1, 2, 5))  # expected income of each career\n",
    "score_orig = 0.5 * income  # scores for each career, based on income\n",
    "p_orig = softmax(score_orig)\n",
    "\n",
    "income_new = np.array((1, 2 * 2, 5))  # expected income of each career\n",
    "score_new = 0.5 * income_new  # scores for each career, based on income\n",
    "p_new = softmax(score_new)\n",
    "\n",
    "print(\"True p_diff:\", p_new[1] - p_orig[1])  # Change in probability of the second career"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.59"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "N = 500\n",
    "\n",
    "# simulate family incomes for each individual\n",
    "family_income = np.random.rand(N)\n",
    "\n",
    "# assign a unique coefficient for each type of event\n",
    "b = np.array([-2.0, 0.0, 2.0])\n",
    "\n",
    "p = softmax(np.array([0.5, 1.0, 1.5])[:, None] + np.outer(b, family_income), axis=0).T\n",
    "\n",
    "career = np.asarray([np.random.multinomial(1, pp) for pp in p])\n",
    "career = np.where(career == 1)[1]\n",
    "career[:11]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with pm.Model() as m11_14:\n",
    "    a = pm.Normal(\"a\", 0.0, 1.5, shape=2)  # intercepts\n",
    "    b = pm.Normal(\"b\", 0.0, 1.0, shape=2)  # coefficients on family income\n",
    "\n",
    "    s0 = a[0] + b[0] * family_income\n",
    "    s1 = a[1] + b[1] * family_income\n",
    "    s2 = np.zeros(N)  # pivot\n",
    "    s = pm.math.stack([s0, s1, s2]).T\n",
    "\n",
    "    p_ = at.nnet.softmax(s)\n",
    "    career_obs = pm.Categorical(\"career\", p=p_, observed=career)\n",
    "\n",
    "    trace_11_14 = pm.sample(1000, tune=2000, target_accept=0.9, random_seed=RANDOM_SEED)\n",
    "az.summary(trace_11_14, round_to=2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.60"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "d_ad = pd.read_csv(\"Data/UCBadmit.csv\", sep=\";\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.61"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# binomial model of overall admission probability\n",
    "with pm.Model() as m_binom:\n",
    "    a = pm.Normal(\"a\", 0, 1.5)\n",
    "    p = pm.math.invlogit(a)\n",
    "\n",
    "    admit = pm.Binomial(\"admit\", p=p, n=d_ad.applications.values, observed=d_ad.admit.values)\n",
    "\n",
    "    trace_binom = pm.sample(1000, tune=2000)\n",
    "\n",
    "# Poisson model of overall admission and rejection rates\n",
    "with pm.Model() as m_pois:\n",
    "    a = pm.Normal(\"a\", 0, 1.5, shape=2)\n",
    "    lam = pm.math.exp(a)\n",
    "\n",
    "    admit = pm.Poisson(\"admit\", lam[0], observed=d_ad.admit.values)\n",
    "    rej = pm.Poisson(\"rej\", lam[1], observed=d_ad.reject.values)\n",
    "\n",
    "    trace_pois = pm.sample(1000, tune=2000)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.62"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "m_binom = az.summary(trace_binom)\n",
    "logistic(m_binom[\"mean\"]).round(3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code 11.63"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "m_pois = az.summary(trace_pois).round(2)\n",
    "(np.exp(m_pois[\"mean\"][0]) / (np.exp(m_pois[\"mean\"][0]) + np.exp(m_pois[\"mean\"][1]))).round(3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11M7.  Use quap to construct a quadratic approximate posterior distribution for the chimpanzee model that includes a unique intercept for each actor, m11.4 (page 330). Compare the quadratic approximation to the posterior distribution produced instead from MCMC. Can you explain both the differences and the similarities between the approximate and the MCMC distributions? Relax the prior on the actor intercepts to Normal(0,10). Re-estimate the posterior using both ulam and quap. Do the differences increase or decrease? Why?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "d = pd.read_csv(\"Data/chimpanzees.csv\", sep=\";\")\n",
    "# we change \"actor\" to zero-index\n",
    "d.actor = d.actor - 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "d[\"treatment\"] = d.prosoc_left + 2 * d.condition\n",
    "d[[\"actor\", \"prosoc_left\", \"condition\", \"treatment\"]]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "actor_idx, actors = pd.factorize(d.actor)\n",
    "treat_idx, treatments = pd.factorize(d.treatment)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with pm.Model() as m11_4:\n",
    "    a = pm.Normal(\"a\", 0.0, 1.5, shape=len(actors))\n",
    "    b = pm.Normal(\"b\", 0.0, 0.5, shape=len(treatments))\n",
    "\n",
    "    actor_id = pm.intX(pm.Data(\"actor_id\", actor_idx))\n",
    "    treat_id = pm.intX(pm.Data(\"treat_id\", treat_idx))\n",
    "    p = pm.Deterministic(\"p\", pm.math.invlogit(a[actor_id] + b[treat_id]))\n",
    "\n",
    "    pulled_left = pm.Binomial(\"pulled_left\", 1, p, observed=d.pulled_left)\n",
    "\n",
    "    trace_11_4 = pm.sample(random_seed=RANDOM_SEED)\n",
    "\n",
    "az.summary(trace_11_4, var_names=[\"a\", \"b\"], round_to=2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#quadratic approximation\n",
    "\n",
    "n_obs = d.shape[0]\n",
    "n_actors = len(actor_idx)\n",
    "n_treatments = len(treat_idx)\n",
    "\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt  # for tensor ops\n",
    "from pytensor import function\n",
    "from pytensor.gradient import hessian\n",
    "from pytensor import function\n",
    "# from pytensor.gradient import hessian\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Define the model\n",
    "with pm.Model() as model:\n",
    "    a = pm.Normal(\"a\", mu=0, sigma=1.5, shape=n_actors)\n",
    "    b = pm.Normal(\"b\", mu=0, sigma=0.5, shape=n_treatments)\n",
    "    \n",
    "    actor_id = pm.ConstantData(\"actor_id\", actor_idx)\n",
    "    treat_id = pm.ConstantData(\"treat_id\", treat_idx)\n",
    "    \n",
    "    p = pm.Deterministic(\"p\", pm.math.sigmoid(a[actor_id] + b[treat_id]))\n",
    "    y = pm.Binomial(\"pulled_left\", n=1, p=p, observed=d.pulled_left)\n",
    "\n",
    "    # Find MAP estimate\n",
    "    map_estimate = pm.find_MAP()\n",
    "\n",
    "    # --- Quadratic approximation using Hessian ---\n",
    "\n",
    "    # Get the value variables corresponding to the free RVs\n",
    "    free_rvs = model.free_RVs\n",
    "    value_vars = [model.rvs_to_values[rv] for rv in free_rvs]\n",
    "    \n",
    "    # 1. Create logp terms\n",
    "    logps = [pm.logp(rv, value_var) for rv, value_var in zip(free_rvs, value_vars)]\n",
    "    \n",
    "    # 2. Total scalar logp\n",
    "    logp_scalar = pt.sum(pt.stack(logps))\n",
    "    \n",
    "    # 3. Flatten all value variables for Hessian\n",
    "    flat_vars = pt.concatenate([pt.reshape(v, (-1,)) for v in value_vars])\n",
    "    \n",
    "    # 4. Compute Hessian\n",
    "    hess_expr = hessian(logp_scalar, flat_vars)\n",
    "    hess_func = function(value_vars, hess_expr)\n",
    "# 4. Evaluate Hessian at MAP\n",
    "flat_map = pm.blockwise_flatten(map_estimate, free_rvs)\n",
    "H = hess_func()\n",
    "\n",
    "# 5. Compute covariance (Laplace approximation)\n",
    "cov = np.linalg.inv(-H)\n",
    "\n",
    "# 6. Sample from multivariate normal approximation\n",
    "mean = np.concatenate([map_estimate[\"a\"], map_estimate[\"b\"]])\n",
    "samples = multivariate_normal.rvs(mean=mean, cov=cov, size=1000)\n",
    "\n",
    "# 7. Convert to DataFrame\n",
    "param_names = [f\"a[{i}]\" for i in range(n_actors)] + [f\"b[{j}]\" for j in range(n_treatments)]\n",
    "approx_df = pd.DataFrame(samples, columns=param_names)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gave up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11M8.  Revisit the data(Kline) islands example. This time drop Hawaii from the sample and refit the models. What changes do you observe?m"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dk = pd.read_csv(\"Data/Kline\", sep=\";\")\n",
    "dk"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dk.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dk_wo_h=dk[dk.culture!='Hawaii']\n",
    "dk_wo_h"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "P_wo_h = standardize(np.log(dk_wo_h.population)).values\n",
    "c_id_wo_h = (dk_wo_h.contact == \"high\").astype(int).values"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "P = standardize(np.log(dk.population)).values\n",
    "c_id = (dk.contact == \"high\").astype(int).values"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# intercept only\n",
    "with pm.Model() as m11_9:\n",
    "    a = pm.Normal(\"a\", 3.0, 0.5)\n",
    "    T = pm.Poisson(\"total_tools\", pm.math.exp(a), observed=dk.total_tools)\n",
    "    trace_11_9 = pm.sample(tune=3000, random_seed=RANDOM_SEED)\n",
    "\n",
    "# interaction model\n",
    "with pm.Model() as m11_10:\n",
    "    a = pm.Normal(\"a\", 3.0, 0.5, shape=2)\n",
    "    b = pm.Normal(\"b\", 0.0, 0.2, shape=2)\n",
    "\n",
    "    cid = pm.intX(pm.Data(\"cid\", c_id))\n",
    "    P_ = pm.Data(\"P\", P)\n",
    "    lam = pm.math.exp(a[cid] + b[cid] * P_)\n",
    "\n",
    "    T = pm.Poisson(\"total_tools\", lam, observed=dk.total_tools)\n",
    "    trace_11_10 = pm.sample(tune=3000, random_seed=RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az.summary(trace_11_9)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az.summary(trace_11_10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# intercept only\n",
    "with pm.Model() as m11_9_wo_h:\n",
    "    a = pm.Normal(\"a\", 3.0, 0.5)\n",
    "    T = pm.Poisson(\"total_tools\", pm.math.exp(a), observed=dk_wo_h.total_tools)\n",
    "    trace_11_9_wo_h = pm.sample(tune=3000, random_seed=RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with pm.Model() as m11_10_wo_h:\n",
    "    a = pm.Normal(\"a\", 3.0, 0.5, shape=2)\n",
    "    b = pm.Normal(\"b\", 0.0, 0.2, shape=2)\n",
    "\n",
    "    cid = pm.intX(pm.Data(\"cid\", c_id_wo_h))\n",
    "    P_ = pm.Data(\"P\", P_wo_h)\n",
    "    lam = pm.Deterministic(\"lam\", pm.math.exp(a[cid] + b[cid] * P_))\n",
    "\n",
    "    T = pm.Poisson(\"total_tools\", lam, observed=dk_wo_h.total_tools)\n",
    "\n",
    "    trace_11_10_wo_h = pm.sample(\n",
    "        tune=3000,\n",
    "        return_inferencedata=True,              #  make sure to return InferenceData\n",
    "        idata_kwargs={\"log_likelihood\": True},  #  tell PyMC to include log_likelihood\n",
    "        random_seed=RANDOM_SEED\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az.summary(trace_11_9_wo_h)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az.summary(trace_11_10_wo_h)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ns = 100\n",
    "P_seq = np.linspace(-1.4, 3.0, ns)\n",
    "\n",
    "with m11_10_wo_h:\n",
    "    pm.set_data({\"cid\": np.array([0] * ns), \"P\": P_seq})\n",
    "    lam0_idata = pm.sample_posterior_predictive(trace_11_10_wo_h, var_names=[\"lam\"])\n",
    "    lam0 = lam0_idata.posterior_predictive[\"lam\"]\n",
    "\n",
    "    pm.set_data({\"cid\": np.array([1] * ns), \"P\": P_seq})\n",
    "    lam1_idata = pm.sample_posterior_predictive(trace_11_10_wo_h, var_names=[\"lam\"])\n",
    "    lam1 = lam1_idata.posterior_predictive[\"lam\"]\n",
    "\n",
    "lmu0, lmu1 = lam0.mean([\"chain\", \"draw\"]), lam1.mean([\"chain\", \"draw\"])\n",
    "\n",
    "# store pareto-k values for plot:\n",
    "k = az.loo(trace_11_10_wo_h, pointwise=True).pareto_k.values"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "_, (ax0, ax1) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# scale point size to Pareto-k:\n",
    "k /= k.max()\n",
    "psize = 250 * k\n",
    "\n",
    "# Plot on standardized log scale:\n",
    "\n",
    "az.plot_hdi(P_seq, lam1, color=\"b\", fill_kwargs={\"alpha\": 0.2}, ax=ax0)\n",
    "ax0.plot(P_seq, lmu1, color=\"b\", alpha=0.7, label=\"high contact mean\")\n",
    "\n",
    "az.plot_hdi(P_seq, lam0, color=\"k\", fill_kwargs={\"alpha\": 0.2}, ax=ax0)\n",
    "ax0.plot(P_seq, lmu0, \"--\", color=\"k\", alpha=0.7, label=\"low contact mean\")\n",
    "\n",
    "# display names and k:\n",
    "mask = k > 0.3\n",
    "labels = dk_wo_h.culture.values[mask]\n",
    "for i, text in enumerate(labels):\n",
    "    ax0.text(\n",
    "        P_wo_h[mask][i] - 0.2,\n",
    "        dk_wo_h.total_tools.values[mask][i] + 4,\n",
    "        f\"{text} ({np.round(k[mask][i], 2)})\",\n",
    "        fontsize=8,\n",
    "    )\n",
    "\n",
    "# display observed data:\n",
    "index = c_id_wo_h == 1\n",
    "ax0.scatter(\n",
    "    P_wo_h[~index],\n",
    "    dk_wo_h.total_tools[~index],\n",
    "    s=psize[~index],\n",
    "    facecolors=\"none\",\n",
    "    edgecolors=\"k\",\n",
    "    alpha=0.8,\n",
    "    lw=1,\n",
    "    label=\"low contact\",\n",
    ")\n",
    "ax0.scatter(P_wo_h[index], dk_wo_h.total_tools[index], s=psize[index], alpha=0.8, label=\"high contact\")\n",
    "ax0.set_xlabel(\"log population (std)\")\n",
    "ax0.set_ylabel(\"total tools\")\n",
    "ax0.legend(fontsize=8, ncol=2)\n",
    "\n",
    "# Plot on natural scale:\n",
    "# unstandardize and exponentiate values of standardized log pop:\n",
    "P_seq = np.linspace(-5.0, 3.0, ns)\n",
    "P_seq = np.exp(P_seq * np.log(dk_wo_h.population.values).std() + np.log(dk_wo_h.population.values).mean())\n",
    "\n",
    "az.plot_hdi(P_seq, lam1, color=\"b\", fill_kwargs={\"alpha\": 0.2}, ax=ax1)\n",
    "ax1.plot(P_seq, lmu1, color=\"b\", alpha=0.7)\n",
    "\n",
    "az.plot_hdi(P_seq, lam0, color=\"k\", fill_kwargs={\"alpha\": 0.2}, ax=ax1)\n",
    "ax1.plot(P_seq, lmu0, \"--\", color=\"k\", alpha=0.7)\n",
    "\n",
    "# display observed data:\n",
    "ax1.scatter(\n",
    "    dk_wo_h.population[~index],\n",
    "    dk_wo_h.total_tools[~index],\n",
    "    s=psize[~index],\n",
    "    facecolors=\"none\",\n",
    "    edgecolors=\"k\",\n",
    "    alpha=0.8,\n",
    "    lw=1,\n",
    ")\n",
    "ax1.scatter(dk_wo_h.population[index], dk_wo_h.total_tools[index], s=psize[index], alpha=0.8)\n",
    "plt.setp(ax1.get_xticklabels(), ha=\"right\", rotation=45)\n",
    "ax1.set_xlim((-10_000, 80_000))\n",
    "ax1.set_xlabel(\"population\")\n",
    "ax1.set_ylabel(\"total tools\");"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11H1.  Use WAIC or PSIS to compare the chimpanzee model that includes a unique intercept for each actor, m11.4 (page 330), to the simpler models fit in the same section. Interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with pm.Model() as m11_4:\n",
    "    a = pm.Normal(\"a\", 0.0, 1.5, shape=len(actors))\n",
    "    b = pm.Normal(\"b\", 0.0, 0.5, shape=len(treatments))\n",
    "\n",
    "    actor_id = pm.intX(pm.Data(\"actor_id\", actor_idx))\n",
    "    treat_id = pm.intX(pm.Data(\"treat_id\", treat_idx))\n",
    "    p = pm.Deterministic(\"p\", pm.math.invlogit(a[actor_id] + b[treat_id]))\n",
    "\n",
    "    pulled_left = pm.Binomial(\"pulled_left\", 1, p, observed=d.pulled_left)\n",
    "\n",
    "    trace_11_4 = pm.sample(random_seed=RANDOM_SEED)\n",
    "\n",
    "az.summary(trace_11_4, var_names=[\"a\", \"b\"], round_to=2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "waic_m11_4 = pm.compute_log_likelihood(trace_11_4, model=m11_4)\n",
    "waic_m11_4 = pm.waic(waic_m11_4)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with pm.Model() as m11_3:\n",
    "    a = pm.Normal(\"a\", 0.0, 1.5)\n",
    "    b = pm.Normal(\"b\", 0.0, 0.5, shape=4)\n",
    "\n",
    "    p = pm.math.invlogit(a + b[d.treatment.values])\n",
    "    pulled_left = pm.Binomial(\"pulled_left\", 1, p, observed=d.pulled_left)\n",
    "\n",
    "    trace_11_3 = pm.sample(random_seed=RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "waic_m11_3 = pm.compute_log_likelihood(trace_11_3, model=m11_3)\n",
    "waic_m11_3 = pm.waic(waic_m11_3)\n",
    "waic_m11_3"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "waic_m11_4"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11H2.  The data contained in library(MASS);data(eagles) are records of salmon pirating attempts by Bald Eagles in Washington State. See ?eagles for details. While one eagle feeds, sometimes another will swoop in and try to steal the salmon from it. Call the feeding eagle the victim and the thief the pirate. Use the available data to build a binomial GLM of successful pirating attempts.\n",
    "\n",
    "(a)  Consider the following model:\n",
    "\n",
    " \n",
    "\n",
    "where y is the number of successful attempts, n is the total number of attempts, P is a dummy variable indicating whether or not the pirate had large body size, V is a dummy variable indicating whether or not the victim had large body size, and finally A is a dummy variable indicating whether or not the pirate was an adult. Fit the model above to the eagles data, using both quap and ulam. Is the quadratic approximation okay?\n",
    "\n",
    "(b)  Now interpret the estimates. If the quadratic approximation turned out okay, then its okay to use the quap estimates. Otherwise stick to ulam estimates. Then plot the posterior predictions. Compute and display both (1) the predicted probability. of success and its 89% interval for each row (i) in the data, as well as (2) the predicted success count. and its 89% interval. What different information does each type of posterior prediction provide?\n",
    "\n",
    "(c)  Now try to improve the model. Consider an interaction between the pirates size and age (immature or adult). Compare this model to the previous one, using WAIC. Interpret."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "eagles_df = pd.DataFrame({\n",
    "    \"y\":  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
    "    \"n\":  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
    "    \"P\":  [\"L\"]*10 + [\"S\"]*10 + [\"L\"]*10 + [\"S\"]*10,\n",
    "    \"V\":  [\"L\"]*5 + [\"S\"]*5 + [\"L\"]*5 + [\"S\"]*5 + [\"L\"]*5 + [\"S\"]*5 + [\"L\"]*5 + [\"S\"]*5,\n",
    "    \"A\":  [\"A\"]*20 + [\"I\"]*20\n",
    "})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "eagles_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "eagles_df['success_rate']=eagles_df.y/eagles_df.n\n",
    "\n",
    "v_idx, vs = pd.factorize(eagles_df.V)\n",
    "p_idx, ps = pd.factorize(eagles_df.P)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "set(p_idx)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with pm.Model() as m_eagles:\n",
    "    a = pm.Normal(\"a\", 0.0, 1.5)\n",
    "    bp = pm.Normal(\"bp\", 0.0, 0.5, shape=len(set(p_idx)))\n",
    "    bv = pm.Normal(\"bv\", 0.0, 0.5, shape=len(set(v_idx)))\n",
    "\n",
    "    v_id = pm.intX(pm.Data(\"v_id\", v_idx))\n",
    "    p_id = pm.intX(pm.Data(\"p_id\", p_idx))\n",
    "    p = pm.Deterministic(\"p\", pm.math.invlogit(a + bp[p_id] + bv[v_id]))\n",
    "    n =pm.Data(\"n\", eagles_df.n)\n",
    "\n",
    "    fishing_success = pm.Binomial(\"fishing_success\", n, p, observed=eagles_df.y)\n",
    "\n",
    "    trace_eagles = pm.sample(random_seed=RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)  Now interpret the estimates. If the quadratic approximation turned out okay, then its okay to use the quap estimates. Otherwise stick to ulam estimates. Then plot the posterior predictions. Compute and display both (1) the predicted probability. of success and its 89% interval for each row (i) in the data, as well as (2) the predicted success count. and its 89% interval. What different information does each type of posterior prediction provide?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az.summary(trace_eagles)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ax = az.plot_forest(trace_eagles,var_names=[\"a\", \"bp\", \"bv\"], combined=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az.plot_trace(trace_eagles)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with m_eagles:\n",
    "    pm.set_data({\n",
    "        \"v_id\": v_idx,\n",
    "        \"p_id\": p_idx,\n",
    "        \"n\": eagles_df.n\n",
    "    })\n",
    "    ppd = pm.sample_posterior_predictive(trace_eagles, var_names=[\"fishing_success\", \"p\"], random_seed=RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ppd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# shape: (chains, draws, observations)\n",
    "p_samples = ppd.posterior_predictive[\"p\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "az.plot_hdi(np.arange(len(eagles_df)), p_samples, hdi_prob=0.89, ax=ax)\n",
    "ax.plot(np.arange(len(eagles_df)), p_samples.mean(dim=[\"chain\", \"draw\"]), \"o\", label=\"Mean probability\")\n",
    "ax.set_xlabel(\"Observation index\")\n",
    "ax.set_ylabel(\"Predicted probability of success\")\n",
    "ax.set_title(\"Predicted success probabilities with 89% HDIs\")\n",
    "ax.legend()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Assuming ppd_p is from sample_posterior_predictive\n",
    "p_samples = ppd_p.posterior_predictive[\"p\"]  # shape: (chain, draw, obs)\n",
    "\n",
    "# Convert to numpy array\n",
    "p_array = p_samples.stack(sample=(\"chain\", \"draw\")).values.T  # shape: (samples, obs)\n",
    "# Assume p_array shape: (samples, n_observations)\n",
    "n_obs = p_array.shape[1]\n",
    "means = p_array.mean(axis=0)\n",
    "hdi_lower, hdi_upper = az.hdi(p_array, hdi_prob=0.89).T  # shape: (n_obs,)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(6, n_obs * 0.4))\n",
    "y_pos = np.arange(n_obs)\n",
    "\n",
    "ax.errorbar(\n",
    "    x=means, \n",
    "    y=y_pos, \n",
    "    xerr=[means - hdi_lower, hdi_upper - means],\n",
    "    fmt=\"o\", \n",
    "    color=\"blue\", \n",
    "    ecolor=\"lightblue\", \n",
    "    capsize=4\n",
    ")\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels([f\"obs {i}\" for i in y_pos])\n",
    "ax.set_xlabel(\"Predicted success probability\")\n",
    "ax.set_title(\"Posterior predictive success probabilities (with 89% HDI)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Assuming ppd_p is from sample_posterior_predictive\n",
    "p_samples = ppd.posterior_predictive[\"fishing_success\"]  # shape: (chain, draw, obs)\n",
    "\n",
    "# Convert to numpy array\n",
    "p_array = p_samples.stack(sample=(\"chain\", \"draw\")).values.T  # shape: (samples, obs)\n",
    "# Assume p_array shape: (samples, n_observations)\n",
    "n_obs = p_array.shape[1]\n",
    "means = p_array.mean(axis=0)\n",
    "hdi_lower, hdi_upper = az.hdi(p_array, hdi_prob=0.89).T  # shape: (n_obs,)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(6, n_obs * 0.4))\n",
    "y_pos = np.arange(n_obs)\n",
    "\n",
    "ax.errorbar(\n",
    "    x=means, \n",
    "    y=y_pos, \n",
    "    xerr=[means - hdi_lower, hdi_upper - means],\n",
    "    fmt=\"o\", \n",
    "    color=\"blue\", \n",
    "    ecolor=\"lightblue\", \n",
    "    capsize=4\n",
    ")\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels([f\"obs {i}\" for i in y_pos])\n",
    "ax.set_xlabel(\"Predicted success count\")\n",
    "ax.set_title(\"Posterior predictive success count (with 89% HDI)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Assuming ppd_p is from sample_posterior_predictive\n",
    "p_samples = ppd.posterior_predictive[\"p\"]  # shape: (chain, draw, obs)\n",
    "\n",
    "# Convert to numpy array\n",
    "p_array = p_samples.stack(sample=(\"chain\", \"draw\")).values.T  # shape: (samples, obs)\n",
    "# Group predictions by pirate size\n",
    "groups = np.unique(p_idx)\n",
    "group_names = [\"small\", \"large\"]\n",
    "group_hdis = []\n",
    "group_means = []\n",
    "\n",
    "for g in groups:\n",
    "    group_samples = p_array[:, p_idx == g].flatten()  # flatten across all matching rows\n",
    "    hdi = az.hdi(group_samples, hdi_prob=0.89)\n",
    "    mean = np.mean(group_samples)\n",
    "    group_hdis.append(hdi)\n",
    "    group_means.append(mean)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "y = np.arange(len(groups))\n",
    "hdil, hdiu = np.array(group_hdis).T\n",
    "ax.errorbar(\n",
    "    x=group_means,\n",
    "    y=y,\n",
    "    xerr=[group_means - hdil, hdiu - group_means],\n",
    "    fmt=\"o\",\n",
    "    capsize=4,\n",
    "    color=\"darkblue\",\n",
    "    ecolor=\"skyblue\"\n",
    ")\n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels([f\"Pirate size: {n}\" for n in group_names])\n",
    "ax.set_xlabel(\"Predicted success probability\")\n",
    "ax.set_title(\"Posterior predictive mean + 89% HDI by Pirate Size\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Assuming ppd_p is from sample_posterior_predictive\n",
    "p_samples = ppd.posterior_predictive[\"p\"]  # shape: (chain, draw, obs)\n",
    "\n",
    "# Convert to numpy array\n",
    "p_array = p_samples.stack(sample=(\"chain\", \"draw\")).values.T  # shape: (samples, obs)\n",
    "# Group predictions by pirate size\n",
    "groups = np.unique(v_idx)\n",
    "group_names = [\"small\", \"large\"]\n",
    "group_hdis = []\n",
    "group_means = []\n",
    "\n",
    "for g in groups:\n",
    "    group_samples = p_array[:, v_idx == g].flatten()  # flatten across all matching rows\n",
    "    hdi = az.hdi(group_samples, hdi_prob=0.89)\n",
    "    mean = np.mean(group_samples)\n",
    "    group_hdis.append(hdi)\n",
    "    group_means.append(mean)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "y = np.arange(len(groups))\n",
    "hdil, hdiu = np.array(group_hdis).T\n",
    "ax.errorbar(\n",
    "    x=group_means,\n",
    "    y=y,\n",
    "    xerr=[group_means - hdil, hdiu - group_means],\n",
    "    fmt=\"o\",\n",
    "    capsize=4,\n",
    "    color=\"darkblue\",\n",
    "    ecolor=\"skyblue\"\n",
    ")\n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels([f\"Pirate size: {n}\" for n in group_names])\n",
    "ax.set_xlabel(\"Predicted success probability\")\n",
    "ax.set_title(\"Posterior predictive mean + 89% HDI by Victim Size\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)  Now try to improve the model. Consider an interaction between the pirates size and age (immature or adult). Compare this model to the previous one, using WAIC. Interpret."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "a_idx, a_s = pd.factorize(eagles_df.A)\n",
    "eagles_df[\"p_a_nteraction\"] = p_idx + 2 * a_idx\n",
    "eagles_df['a_idx']=a_idx\n",
    "eagles_df['p_idx']=p_idx\n",
    "eagles_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "eagles_df.p_a_nteraction.value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "eagles_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with pm.Model() as m_eagles_inter:\n",
    "    a = pm.Normal(\"a\", 0.0, 1.5)\n",
    "    bp = pm.Normal(\"bp\", 0.0, 0.5, shape=len(set(p_idx)))\n",
    "    ba = pm.Normal(\"ba\", 0.0, 0.5, shape=len(set(a_idx)))\n",
    "    b_ap =  pm.Normal(\"b_bp\", 0.0, 0.5, shape=len(set(eagles_df[\"p_a_nteraction\"])))\n",
    "\n",
    "    a_id = pm.intX(pm.Data(\"a_id\", a_idx))\n",
    "    p_id = pm.intX(pm.Data(\"p_id\", p_idx))\n",
    "    pa_id = pm.intX(pm.Data(\"pa_id\", eagles_df.p_a_nteraction))\n",
    "    p = pm.Deterministic(\"p\", pm.math.invlogit(a + bp[p_id] + ba[a_id] + b_ap[pa_id]))\n",
    "    n =pm.Data(\"n\", eagles_df.n)\n",
    "\n",
    "    fishing_success = pm.Binomial(\"fishing_success\", n, p, observed=eagles_df.y)\n",
    "\n",
    "    trace_eagles_inter = pm.sample(random_seed=RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az.plot_trace(trace_eagles_inter)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az.summary(trace_eagles_inter)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with pm.Model() as m_eagles_inter:\n",
    "    a = pm.Normal(\"a\", 0.0, 1.5)\n",
    "    bp = pm.Normal(\"bp\", 0.0, 0.5, shape=len(set(p_idx)))\n",
    "    ba = pm.Normal(\"ba\", 0.0, 0.5, shape=len(set(a_idx)))\n",
    "    b_ap =  pm.Normal(\"b_bp\", 0.0, 0.5, shape=len(set(eagles_df[\"p_a_nteraction\"])))\n",
    "\n",
    "    a_id = pm.intX(pm.Data(\"a_id\", a_idx))\n",
    "    p_id = pm.intX(pm.Data(\"p_id\", p_idx))\n",
    "    pa_id = pm.intX(pm.Data(\"pa_id\", eagles_df.p_a_nteraction))\n",
    "    p = pm.Deterministic(\"p\", pm.math.invlogit(a + bp[p_id] + ba[a_id] + b_ap[pa_id]))\n",
    "    n =pm.Data(\"n\", eagles_df.n)\n",
    "\n",
    "    fishing_success = pm.Binomial(\"fishing_success\", n, p, observed=eagles_df.y)\n",
    "    prior_m_eagles_inter = pm.sample_prior_predictive(random_seed=RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "prior_m_eagles_inter.prior['b_bp']\n",
    "az.plot_density(prior_m_eagles_inter.prior, var_names=['b_bp'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pa_id"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with m_eagles_inter:\n",
    "    pm.set_data({\n",
    "        \"pa_id\": eagles_df.p_a_nteraction,\n",
    "        \"p_id\": p_idx,\n",
    "        \"n\": eagles_df.n\n",
    "    })\n",
    "    ppd_inter = pm.sample_posterior_predictive(trace_eagles, var_names=[\"fishing_success\", \"p\"], random_seed=RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Assuming ppd_p is from sample_posterior_predictive\n",
    "p_samples = ppd_inter.posterior_predictive[\"fishing_success\"]  # shape: (chain, draw, obs)\n",
    "\n",
    "# Convert to numpy array\n",
    "p_array = p_samples.stack(sample=(\"chain\", \"draw\")).values.T  # shape: (samples, obs)\n",
    "# Assume p_array shape: (samples, n_observations)\n",
    "n_obs = p_array.shape[1]\n",
    "means = p_array.mean(axis=0)\n",
    "hdi_lower, hdi_upper = az.hdi(p_array, hdi_prob=0.89).T  # shape: (n_obs,)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(6, n_obs * 0.4))\n",
    "y_pos = np.arange(n_obs)\n",
    "\n",
    "ax.errorbar(\n",
    "    x=means, \n",
    "    y=y_pos, \n",
    "    xerr=[means - hdi_lower, hdi_upper - means],\n",
    "    fmt=\"o\", \n",
    "    color=\"blue\", \n",
    "    ecolor=\"lightblue\", \n",
    "    capsize=4\n",
    ")\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels([f\"obs {i}\" for i in y_pos])\n",
    "ax.set_xlabel(\"Predicted success count\")\n",
    "ax.set_title(\"Posterior predictive success count (with 89% HDI)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "eagles_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Assuming ppd_p is from sample_posterior_predictive\n",
    "p_samples = ppd_inter.posterior_predictive[\"p\"]  # shape: (chain, draw, obs)\n",
    "\n",
    "# Convert to numpy array\n",
    "p_array = p_samples.stack(sample=(\"chain\", \"draw\")).values.T  # shape: (samples, obs)\n",
    "# Group predictions by pirate size\n",
    "groups = np.unique(eagles_df.p_a_nteraction)\n",
    "group_names = [\"Age=A, Body=Small\", \"Age=A, Body=Large\",\"Age=I, Body=Small\" ,\"Age=I, Body=Large\"]\n",
    "group_hdis = []\n",
    "group_means = []\n",
    "\n",
    "for g in groups:\n",
    "    group_samples = p_array[:, eagles_df.p_a_nteraction == g].flatten()  # flatten across all matching rows\n",
    "    hdi = az.hdi(group_samples, hdi_prob=0.89)\n",
    "    mean = np.mean(group_samples)\n",
    "    group_hdis.append(hdi)\n",
    "    group_means.append(mean)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "y = np.arange(len(groups))\n",
    "hdil, hdiu = np.array(group_hdis).T\n",
    "ax.errorbar(\n",
    "    x=group_means,\n",
    "    y=y,\n",
    "    xerr=[group_means - hdil, hdiu - group_means],\n",
    "    fmt=\"o\",\n",
    "    capsize=4,\n",
    "    color=\"darkblue\",\n",
    "    ecolor=\"skyblue\"\n",
    ")\n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels([f\"Pirate size: {n}\" for n in group_names])\n",
    "ax.set_xlabel(\"Predicted success probability\")\n",
    "ax.set_title(\"Posterior predictive mean + 89% HDI by Pirate Size\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "waic_eagles_inter = pm.compute_log_likelihood(trace_eagles_inter, model=m_eagles_inter)\n",
    "waic_eagles_inter = pm.waic(waic_eagles_inter)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "waic_eagles = pm.compute_log_likelihood(trace_eagles, model=m_eagles)\n",
    "waic_eagles = pm.waic(waic_eagles)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "waic_eagles_inter"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "waic_eagles"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks like that interaction model has improved the prediction accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11H3.  The data contained in data(salamanders) are counts of salamanders (Plethodon elongatus) from 47 different 49-m2 plots in northern California.181 The column SALAMAN is the count in each plot, and the columns PCTCOVER and FORESTAGE are percent of ground cover and age of trees in the plot, respectively. You will model SALAMAN as a Poisson variable.\n",
    "\n",
    "(a)  Model the relationship between density and percent cover, using a log-link (same as the example in the book and lecture). Use weakly informative priors of your choosing. Check the quadratic approximation again, by comparing quap to ulam. Then plot the expected counts and their 89% interval against percent cover. In which ways does the model do a good job? A bad job?\n",
    "\n",
    "(b)  Can you improve the model by using the other predictor, FORESTAGE? Try any models you think useful. Can you explain why FORESTAGE helps or does not help with prediction?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_salam=pd.read_csv('End_of_chapter_problems/data/salamanders.csv', sep=';')\n",
    "df_salam.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_salam.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_salam.SALAMAN.hist()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_salam.SALAMAN.mean(), np.log(df_salam.SALAMAN.mean())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_salam.SALAMAN.value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_salam.PCTCOVER.hist()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_salam.FORESTAGE.hist()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_salam['PCTCOVER_std']=standardize(df_salam.PCTCOVER)\n",
    "df_salam['FORESTAGE_std']=standardize(df_salam.FORESTAGE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with pm.Model() as m_salam:\n",
    "    a = pm.Normal(\"a\", 0.0, 0.6)\n",
    "    b_cover = pm.Normal(\"b_cover\", 0.0, 0.2)\n",
    "    # b_forestage = pm.Normal(\"b_forestage\", 0.0, 0.2)\n",
    "    \n",
    "    cover_data = pm.Data(\"cover_data\", df_salam.PCTCOVER_std)\n",
    "    # forestage_data = pm.Data(\"forestage_data\", df_salam.FORESTAGE_std)\n",
    "    # lam = pm.math.exp(a + b_cover * cover_data + b_forestage*forestage_data)\n",
    "    lam = pm.math.exp(a + b_cover * cover_data)\n",
    "\n",
    "    salam_count = pm.Poisson(\"salam_count\", lam, observed=df_salam.SALAMAN)\n",
    "    trace_m_salam = pm.sample(random_seed=RANDOM_SEED)\n",
    "    prior_m_salam = pm.sample_prior_predictive(random_seed=RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### priors"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az_prior = az.from_dict(prior_predictive={\"salam_count\": prior_m_salam.prior_predictive[\"salam_count\"]})\n",
    "\n",
    "az.plot_dist(\n",
    "    az_prior.prior_predictive[\"salam_count\"].stack(samples=(\"chain\", \"draw\")),\n",
    "    kind=\"hist\",\n",
    "    hist_kwargs={'bins':10},\n",
    "    color=\"lightcoral\",\n",
    "    backend_kwargs={\"figsize\": (7, 4)}\n",
    ")\n",
    "plt.xlabel(\"Simulated salamander counts\")\n",
    "plt.title(\"Prior Predictive Distribution of Salamander Counts\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az.plot_density(prior_m_salam.prior['a'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az.plot_density(prior_m_salam.prior['b_cover'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az_prior = az.from_dict(prior={k: v for k, v in prior_m_salam.prior.items()})\n",
    "\n",
    "az.plot_posterior(\n",
    "    az_prior,\n",
    "    var_names=[\"a\", \"b_cover\"],\n",
    "    kind=\"hist\",\n",
    "    hdi_prob=0.89,\n",
    "    group=\"prior\",\n",
    "    figsize=(8, 3)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az.plot_trace(trace_m_salam)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az.summary(trace_m_salam2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "P_seq = np.linspace(df_salam[\"PCTCOVER_std\"].min(), df_salam[\"PCTCOVER_std\"].max(), 47)\n",
    "\n",
    "with m_salam:\n",
    "    pm.set_data({\"cover_data\": P_seq})\n",
    "    lam_pred = pm.sample_posterior_predictive(trace_m_salam, var_names=[\"salam_count\"])['posterior_predictive']['salam_count']\n",
    "\n",
    "# Compute mean and 89% HDI\n",
    "mean_lam = lam_pred.mean(dim=('chain', 'draw'))\n",
    "hdi_lam = az.hdi(lam_pred, hdi_prob=0.89)\n",
    "\n",
    "# Get HDI as NumPy array from the xarray DataArray\n",
    "hdi_array = hdi_lam[\"salam_count\"].values\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(P_seq, mean_lam, label=\"Expected count\")\n",
    "plt.fill_between(P_seq, hdi_array[:, 0], hdi_array[:, 1], alpha=0.3, label=\"89% HDI\")\n",
    "plt.xlabel(\"Standardized % cover\")\n",
    "plt.ylabel(\"Expected salamander count\")\n",
    "plt.legend()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)  Can you improve the model by using the other predictor, FORESTAGE? Try any models you think useful. Can you explain why FORESTAGE helps or does not help with prediction?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with pm.Model() as m_salam2:\n",
    "    a = pm.Normal(\"a\", 0.0, 0.6)\n",
    "    b_cover = pm.Normal(\"b_cover\", 0.0, 0.2)\n",
    "    b_forestage = pm.Normal(\"b_forestage\", 0.0, 0.2)\n",
    "    \n",
    "    cover_data = pm.Data(\"cover_data\", df_salam.PCTCOVER_std)\n",
    "    forestage_data = pm.Data(\"forestage_data\", df_salam.FORESTAGE_std)\n",
    "    lam = pm.math.exp(a + b_cover * cover_data + b_forestage*forestage_data)\n",
    "    # lam = pm.math.exp(a + b_cover * cover_data)\n",
    "\n",
    "    salam_count = pm.Poisson(\"salam_count\", lam, observed=df_salam.SALAMAN)\n",
    "    trace_m_salam2 = pm.sample(random_seed=RANDOM_SEED)\n",
    "    prior_m_salam2 = pm.sample_prior_predictive(random_seed=RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az_prior = az.from_dict(prior_predictive={\"salam_count\": prior_m_salam2.prior_predictive[\"salam_count\"]})\n",
    "\n",
    "az.plot_dist(\n",
    "    az_prior.prior_predictive[\"salam_count\"].stack(samples=(\"chain\", \"draw\")),\n",
    "    kind=\"hist\",\n",
    "    hist_kwargs={'bins':10},\n",
    "    color=\"lightcoral\",\n",
    "    backend_kwargs={\"figsize\": (7, 4)}\n",
    ")\n",
    "plt.xlabel(\"Simulated salamander counts\")\n",
    "plt.title(\"Prior Predictive Distribution of Salamander Counts\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az_prior = az.from_dict(prior={k: v for k, v in prior_m_salam2.prior.items()})\n",
    "\n",
    "az.plot_posterior(\n",
    "    az_prior,\n",
    "    var_names=[\"a\", \"b_cover\", \"b_forestage\"],\n",
    "    kind=\"hist\",\n",
    "    hdi_prob=0.89,\n",
    "    group=\"prior\",\n",
    "    figsize=(8, 3)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az.plot_trace(trace_m_salam2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az.summary(trace_m_salam2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "P_seq = np.linspace(df_salam[\"PCTCOVER_std\"].min(), df_salam[\"PCTCOVER_std\"].max(), 47)\n",
    "\n",
    "with m_salam2:\n",
    "    pm.set_data({\"cover_data\": P_seq})\n",
    "    lam_pred = pm.sample_posterior_predictive(trace_m_salam2, var_names=[\"salam_count\"])['posterior_predictive']['salam_count']\n",
    "\n",
    "# Compute mean and 89% HDI\n",
    "mean_lam = lam_pred.mean(dim=('chain', 'draw'))\n",
    "hdi_lam = az.hdi(lam_pred, hdi_prob=0.89)\n",
    "\n",
    "# Get HDI as NumPy array from the xarray DataArray\n",
    "hdi_array = hdi_lam[\"salam_count\"].values\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(P_seq, mean_lam, label=\"Expected count\")\n",
    "plt.fill_between(P_seq, hdi_array[:, 0], hdi_array[:, 1], alpha=0.3, label=\"89% HDI\")\n",
    "plt.xlabel(\"Standardized cover\")\n",
    "plt.ylabel(\"Expected salamander count\")\n",
    "plt.legend()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "P_seq = np.linspace(df_salam[\"FORESTAGE_std\"].min(), df_salam[\"FORESTAGE_std\"].max(), 47)\n",
    "\n",
    "with m_salam2:\n",
    "    pm.set_data({\"forestage_data\": P_seq})\n",
    "    lam_pred = pm.sample_posterior_predictive(trace_m_salam2, var_names=[\"salam_count\"])['posterior_predictive']['salam_count']\n",
    "\n",
    "# Compute mean and 89% HDI\n",
    "mean_lam = lam_pred.mean(dim=('chain', 'draw'))\n",
    "hdi_lam = az.hdi(lam_pred, hdi_prob=0.89)\n",
    "\n",
    "# Get HDI as NumPy array from the xarray DataArray\n",
    "hdi_array = hdi_lam[\"salam_count\"].values\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(P_seq, mean_lam, label=\"Expected count\")\n",
    "plt.fill_between(P_seq, hdi_array[:, 0], hdi_array[:, 1], alpha=0.3, label=\"89% HDI\")\n",
    "plt.xlabel(\"Standardized forest age\")\n",
    "plt.ylabel(\"Expected salamander count\")\n",
    "plt.legend()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "waic_salam = pm.compute_log_likelihood(trace_m_salam, model=m_salam)\n",
    "waic_salam = pm.waic(waic_salam)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "waic_salam2 = pm.compute_log_likelihood(trace_m_salam2, model=m_salam2)\n",
    "waic_salam2 = pm.waic(waic_salam2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "waic_salam"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "waic_salam2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If too many k values > 0.7, prefer PSIS-LOO over WAIC using az.loo()."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compute LOO (leave-one-out cross-validation)\n",
    "loo_result = az.loo(trace_m_salam, pointwise=True)\n",
    "\n",
    "# Now plot the Pareto-k diagnostics\n",
    "az.plot_khat(loo_result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compute LOO (leave-one-out cross-validation)\n",
    "loo_result2 = az.loo(trace_m_salam2, pointwise=True)\n",
    "\n",
    "# Now plot the Pareto-k diagnostics\n",
    "az.plot_khat(loo_result2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "loo_result"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "loo_result2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with pm.Model() as m_salam3:\n",
    "    a = pm.Normal(\"a\", 0.0, 0.6)\n",
    "    b_cover = pm.Normal(\"b_cover\", 0.0, 0.2)\n",
    "    b_forestage = pm.Normal(\"b_forestage\", 0.0, 0.2)\n",
    "    b_interact = pm.Normal(\"b_interact\", 0.0, 0.1)\n",
    "    \n",
    "    cover_data = pm.Data(\"cover_data\", df_salam.PCTCOVER_std)\n",
    "    forestage_data = pm.Data(\"forestage_data\", df_salam.FORESTAGE_std)\n",
    "    lam_ = pm.math.exp(a + b_cover * cover_data + b_forestage*forestage_data+ b_interact*cover_data*forestage_data)\n",
    "    lam = pm.Deterministic(\"lam\", lam)\n",
    "\n",
    "    salam_count = pm.Poisson(\"salam_count\", lam_, observed=df_salam.SALAMAN)\n",
    "    trace_m_salam3 = pm.sample(random_seed=RANDOM_SEED)\n",
    "    prior_m_salam3 = pm.sample_prior_predictive(random_seed=RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az.summary(trace_m_salam3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az.plot_trace(trace_m_salam3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "P_seq = np.linspace(df_salam[\"PCTCOVER_std\"].min(), df_salam[\"PCTCOVER_std\"].max(), 47)\n",
    "\n",
    "with m_salam3:\n",
    "    pm.set_data({\"cover_data\": P_seq})\n",
    "    lam_pred = pm.sample_posterior_predictive(trace_m_salam3, var_names=[\"salam_count\"])['posterior_predictive']['salam_count']\n",
    "\n",
    "# Compute mean and 89% HDI\n",
    "mean_lam = lam_pred.mean(dim=('chain', 'draw'))\n",
    "hdi_lam = az.hdi(lam_pred, hdi_prob=0.89)\n",
    "\n",
    "# Get HDI as NumPy array from the xarray DataArray\n",
    "hdi_array = hdi_lam[\"salam_count\"].values\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(P_seq, mean_lam, label=\"Expected count\")\n",
    "plt.fill_between(P_seq, hdi_array[:, 0], hdi_array[:, 1], alpha=0.3, label=\"89% HDI\")\n",
    "plt.xlabel(\"Standardized cover\")\n",
    "plt.ylabel(\"Expected salamander count\")\n",
    "plt.legend()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compute LOO (leave-one-out cross-validation)\n",
    "waic_salam3 = pm.compute_log_likelihood(trace_m_salam3, model=m_salam3)\n",
    "loo_result3 = az.loo(trace_m_salam3, pointwise=True)\n",
    "\n",
    "# Now plot the Pareto-k diagnostics\n",
    "az.plot_khat(loo_result3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "loo_result3"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#nteraction grid, need this model wo observed data because it has then fixed number of datapoints\n",
    "with pm.Model() as m_salam3_pred:\n",
    "    a = pm.Normal(\"a\", 0.0, 0.6)\n",
    "    b_cover = pm.Normal(\"b_cover\", 0.0, 0.2)\n",
    "    b_forestage = pm.Normal(\"b_forestage\", 0.0, 0.2)\n",
    "    b_interact = pm.Normal(\"b_interact\", 0.0, 0.1)\n",
    "    \n",
    "    cover_data = pm.Data(\"cover_data\", cover_flat)\n",
    "    forestage_data = pm.Data(\"forestage_data\", forestage_flat)\n",
    "    \n",
    "    lam = pm.math.exp(a + b_cover * cover_data + b_forestage * forestage_data + b_interact * cover_data * forestage_data)\n",
    "    \n",
    "    salam_count = pm.Poisson(\"salam_count\", lam)  # <- no observed here!\n",
    "\n",
    "    # Use previously sampled posterior\n",
    "    lam_pred = pm.sample_posterior_predictive(trace_m_salam3, var_names=[\"salam_count\"], random_seed=RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create grid of standardized cover and forestage\n",
    "cover_grid = np.linspace(df_salam[\"PCTCOVER_std\"].min(), df_salam[\"PCTCOVER_std\"].max(), 30)\n",
    "forestage_grid = np.linspace(df_salam[\"FORESTAGE_std\"].min(), df_salam[\"FORESTAGE_std\"].max(), 30)\n",
    "cover_mesh, forestage_mesh = np.meshgrid(cover_grid, forestage_grid)\n",
    "\n",
    "# Flatten for prediction\n",
    "cover_flat = cover_mesh.ravel()\n",
    "forestage_flat = forestage_mesh.ravel()\n",
    "\n",
    "# Extract the posterior predictive mean\n",
    "mean_pred = lam_pred[\"posterior_predictive\"][\"salam_count\"].mean(axis=(0, 1))  # shape (900,)\n",
    "mean_grid = mean_pred.values.reshape(cover_mesh.shape)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "contour = plt.contourf(cover_mesh, forestage_mesh, mean_grid, levels=20, cmap=\"viridis\")\n",
    "cbar = plt.colorbar(contour)\n",
    "cbar.set_label(\"Expected salamander count\")\n",
    "plt.xlabel(\"Standardized % Cover\")\n",
    "plt.ylabel(\"Standardized Forest Age\")\n",
    "plt.title(\"Interaction Effect: Cover  Forest Age\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interaction model increases prediction accuracy but interaction term is negative meaning that if forest is older or coverage is wider it reduces a bit other term effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11H4.  The data in data(NWOGrants) are outcomes for scientific funding applications for the Netherlands Organization for Scientific Research (NWO) from 20102012 (see van der Lee and Ellemers (2015) for data and context). These data have a very similar structure to the UCBAdmit data discussed in the chapter. I want you to consider a similar question: What are the total and indirect causal effects of gender on grant awards? Consider a mediation path (a pipe) through discipline. Draw the corresponding DAG and then use one or more binomial GLMs to answer the question. What is your causal interpretation? If NWOs goal is to equalize rates of funding between men and women, what type of intervention would be most effective?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_grants=pd.read_csv('End_of_chapter_problems/data/NWOGrants.csv', sep=';')\n",
    "df_grants.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_grants.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_grants['awards_p']=df_grants.awards/df_grants.applications\n",
    "disc_ids = pd.Categorical(df_grants[\"discipline\"]).codes\n",
    "gender_ids = pd.Categorical(df_grants[\"gender\"]).codes\n",
    "df_grants['gender_id']=gender_ids\n",
    "df_grants['discipline_id']=disc_ids\n",
    "# gender_id"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_grants.groupby('gender')['awards_p'].mean()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "summary = df_grants.groupby(\"gender\").agg(\n",
    "    total_applications=(\"applications\", \"sum\"),\n",
    "    total_awards=(\"awards\", \"sum\")\n",
    ")\n",
    "summary[\"success_rate\"] = summary[\"total_awards\"] / summary[\"total_applications\"]\n",
    "\n",
    "summary"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_grants.groupby('gender')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_grants.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "len(np.unique(gender_ids))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "(df_grants.awards_p).mean()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#a prior is in logit scale\n",
    "logit(0.19)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_grants.awards_p"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with pm.Model() as m_grants1:\n",
    "    a = pm.Normal(\"a\", -1.5, 0.5, shape=len(np.unique(gender_ids)))\n",
    "    gender_id=pm.Data(\"gender_id\", df_grants['gender_id'])\n",
    "    p = pm.Deterministic(\"p\", pm.math.invlogit(a[gender_id]))\n",
    "    \n",
    "    admit = pm.Binomial(\"award_p\", p=p, n=df_grants.applications.values, observed=df_grants.awards)\n",
    "\n",
    "    trace_m_grants1 = pm.sample(2000, random_seed=RANDOM_SEED)\n",
    "    prior_m_grants1 = pm.sample_prior_predictive(random_seed=RANDOM_SEED)\n",
    "    \n",
    "az.summary(trace_m_grants1, round_to=2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az_prior = az.from_dict(prior={k: v for k, v in prior_m_grants1.prior.items()})\n",
    "\n",
    "az.plot_posterior(\n",
    "    az_prior,\n",
    "    var_names=[\"a\", \"p\"],\n",
    "    kind=\"hist\",\n",
    "    hdi_prob=0.89,\n",
    "    group=\"prior\",\n",
    "    figsize=(12, 7)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "male_prob = expit(trace_m_grants1.posterior[\"a\"].sel(a_dim_0=1)).mean().values\n",
    "female_prob = expit(trace_m_grants1.posterior[\"a\"].sel(a_dim_0=0)).mean().values\n",
    "\n",
    "print(f\"female  {female_prob:.3f}, male  {male_prob:.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with pm.Model() as m_grants2:\n",
    "    a = pm.Normal(\"a\", -1.5, 0.5, shape=len(np.unique(gender_ids)))\n",
    "    b_disc = pm.Normal(\"b_disc\", 0, 0.5, shape=len(np.unique(disc_ids)))\n",
    "\n",
    "    gender_id=pm.Data(\"gender_id\", df_grants['gender_id'])\n",
    "    disc_id=pm.Data(\"disc_id\", df_grants['discipline_id'])\n",
    "    p = pm.math.invlogit(a[gender_id] + b_disc[disc_id] )\n",
    "\n",
    "    admit = pm.Binomial(\"award_p\", p=p, n=df_grants.applications.values, observed=df_grants.awards)\n",
    "\n",
    "    trace_m_grants2 = pm.sample(2000, random_seed=RANDOM_SEED)\n",
    "    \n",
    "az.summary(trace_m_grants2, round_to=2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#b_disc[0]\n",
    "expit(0.38)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#b_disc[7]\n",
    "expit(-0.34)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az.plot_forest(trace_m_grants2, combined=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az.plot_forest(trace_m_grants1, combined=True, var_names=[\"a\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az.plot_forest(trace_m_grants2, combined=True, var_names=[\"a\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "male_prob = expit(trace_m_grants2.posterior[\"a\"].sel(a_dim_0=1)).mean().values\n",
    "female_prob = expit(trace_m_grants2.posterior[\"a\"].sel(a_dim_0=0)).mean().values\n",
    "\n",
    "print(f\"female  {female_prob:.3f}, male  {male_prob:.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_grants[df_grants.discipline_id.isin([0,7])]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in male total gender effect is 0.177, but if controlling for discipline it is 0.192, which means that discipline might amplify some of the effects.  It is similar with female from 0.150 to 0.171. Alternative explanation is that there could be some third variable that affects result and gender (directly or via mediation). Male awards probability tends to be higher than female one. From disciplines 7 and 0 seem to have coefficients above/below 0. Discipline with  0 index is \"Chemical sciences\" and with index 7 is \"Social sciences\". In first award p tends to be higher in latter lower (note these coeffiecients are pooled, information is shared between disciplines). Most effective interventions would be increase number of women in the disciplines that have more awards given out."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with pm.Model() as m_grants3:\n",
    "    a = pm.Normal(\"a\", -1.5, 0.5, shape=len(np.unique(gender_ids)))\n",
    "    b_disc = pm.Normal(\"b_disc\", 0, 0.5, shape=len(np.unique(disc_ids)))\n",
    "    b_inter = pm.Normal(\"b_inter\", 0, 0.3, shape=(len(np.unique(gender_ids)), len(np.unique(disc_ids))))  # interaction\n",
    "\n",
    "    gender_id=pm.Data(\"gender_id\", df_grants['gender_id'])\n",
    "    disc_id=pm.Data(\"disc_id\", df_grants['discipline_id'])\n",
    "\n",
    "    p = pm.math.invlogit(a[gender_id] + b_disc[disc_id]+ b_inter[gender_id, disc_id] )\n",
    "\n",
    "    admit = pm.Binomial(\"award_p\", p=p, n=df_grants.applications.values, observed=df_grants.awards)\n",
    "\n",
    "    trace_m_grants3 = pm.sample(2000, random_seed=RANDOM_SEED)\n",
    "    \n",
    "az.summary(trace_m_grants3, round_to=2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#interaction term coefficients were not significant"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11H5.  Suppose that the NWO Grants sample has an unobserved confound that influences both choice of discipline and the probability of an award. One example of such a confound could be the career stage of each applicant. Suppose that in some disciplines, junior scholars apply for most of the grants. In other disciplines, scholars from all career stages compete. As a result, career stage influences discipline as well as the probability of being awarded a grant. Add these influences to your DAG from the previous problem. What happens now when you condition on discipline? Does it provide an un-confounded estimate of the direct path from gender to an award? Why or why not? Justify your answer with the backdoor criterion. If you have trouble thinking this though, try simulating fake data, assuming your DAG is true. Then analyze it using the model from the previous problem. What do you conclude? Is it possible for gender to have a real direct causal influence but for a regression conditioning on both gender and discipline to suggest zero influence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditioning on discipline without also adjusting for career stage leads to a confounded estimate of the direct effect of gender on awards. According to the backdoor criterion, we must block all backdoor paths from the treatment (gender) to the outcome (award), and here, the only way to do that is to also condition on career stage  which we cannot do if it is unobserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11H6.  The data in data(Primates301) are 301 primate species and associated measures. In this problem, you will consider how brain size is associated with social learning. There are three parts.\n",
    "\n",
    "(a)  Model the number of observations of social_learning for each species as a function of the log brain size. Use a Poisson distribution for the social_learning outcome variable. Interpret the resulting posterior. (b) Some species are studied much more than others. So the number of reported instances of social_learning could be a product of research effort. Use the research_effort variable, specifically its logarithm, as an additional predictor variable. Interpret the coefficient for log research_effort. How does this model differ from the previous one? (c) Draw a DAG to represent how you think the variables social_learning, brain, and research_effort interact. Justify the DAG with the measured associations in the two models above (and any other models you used)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_primates=pd.read_csv('Data/Primates301.csv', sep=';')\n",
    "df_primates.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_primates.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_primates.research_effort.hist()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_primates.brain.hist()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_primates['research_effort_log']=np.log(df_primates.research_effort)\n",
    "df_primates['brain_log']=np.log(df_primates.brain)\n",
    "\n",
    "df_primates.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_primates.shape\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_primates=df_primates[~pd.isnull(df_primates.brain_log)]\n",
    "df_primates.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_primates.social_learning.hist()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_primates.social_learning.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with pm.Model() as m_social_learning:\n",
    "    a = pm.Normal(\"a\", 0.0, 1.0)\n",
    "    b_brain = pm.Normal(\"b_brain\", 0.0, 0.5)\n",
    "\n",
    "    brain_log = pm.Data(\"brain_log\", df_primates.brain_log)\n",
    "    lam = pm.math.exp(a+ b_brain * brain_log)\n",
    "\n",
    "    social_learning = pm.Poisson(\"social_learning\", lam, observed=df_primates.social_learning)\n",
    "    trace_m_social_learning = pm.sample(random_seed=RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "prior_pred.prior_predictive"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with m_social_learning:\n",
    "    prior_pred = pm.sample_prior_predictive()\n",
    "    az.plot_dist(prior_pred.prior_predictive[\"social_learning_observed\"].stack(samples=(\"chain\", \"draw\")), kind=\"hist\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az.summary(trace_m_social_learning)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One unit increase of brain on log scale increase social learning np.exp(2.115) (about 8.29) units"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az.plot_trace(trace_m_social_learning)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_primates=df_primates[~pd.isnull(df_primates.research_effort_log)]\n",
    "df_primates.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with pm.Model() as m_social_learning2:\n",
    "    a = pm.Normal(\"a\", 0.0, 1.0)\n",
    "    b_brain = pm.Normal(\"b_brain\", 0.0, 0.5)\n",
    "    b_effort = pm.Normal(\"b_effort\", 0.0, 0.5)\n",
    "\n",
    "    brain_log = pm.Data(\"brain_log\", df_primates.brain_log)\n",
    "    effort_log = pm.Data(\"effort_log\", df_primates.research_effort_log)\n",
    "    lam = pm.math.exp(a+ b_brain * brain_log+ b_effort*effort_log)\n",
    "\n",
    "    social_learning = pm.Poisson(\"social_learning\", lam, observed=df_primates.social_learning)\n",
    "    trace_m_social_learning2 = pm.sample(random_seed=RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "np.exp(1.584)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "az.summary(trace_m_social_learning2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for effort coefficient now means that each 1 log unit of increase in effort it increases np.exp(1.584) (4.8 times) social learning count on average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " (c) Draw a DAG to represent how you think the variables social_learning, brain, and research_effort interact. Justify the DAG with the measured associations in the two models above (and any other models you used)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAG could be Brain > Social learning, Brain > Research effort > Social Learning. Because more bigger brains mean more research effort. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two ways of modelling multinomial data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Comparison Summary\n",
    "\n",
    "| **Feature**                          | **Multinomial (Categorical GLM)**       | **Poisson Transformation**             |\n",
    "|--------------------------------------|------------------------------------------|----------------------------------------|\n",
    "| **Distribution**                     | Categorical / Multinomial                | Multiple independent Poissons          |\n",
    "| **Link function**                    | Softmax (multinomial logit)              | Log                                    |\n",
    "| **Outcome data type**                | One outcome per row (id-level)           | Count per category (grouped data)      |\n",
    "| **Handles probabilities**            | Directly                                 | Indirectly (via normalized rates)      |\n",
    "| **Need to model \\(K - 1\\) categories** | Yes (requires a pivot)                  | No (model all \\(K\\) rates separately)  |\n",
    "| **Conditioning on total count**      | No (built-in to likelihood)              | Yes (to recover multinomial behavior)  |\n",
    "| **Interpreting coefficients**        | Tricky (log-odds relative to pivot)      | Log-rate scale (more interpretable)    |\n",
    "\n",
    "---\n",
    "\n",
    "##  When to Use Which?\n",
    "\n",
    "###  Use **Multinomial GLM** when:\n",
    "- You want probabilities directly.\n",
    "- You have individual-level outcome data.\n",
    "- Youre modeling **choice behavior**.\n",
    "\n",
    "###  Use **Poisson transformation** when:\n",
    "- You have **grouped counts** per category.\n",
    "- You prefer modeling **log-rates**.\n",
    "- You want **simpler computation** or access to **Poisson GLM tools**.\n",
    "\n",
    "---\n",
    "\n",
    "##  Why do both give the same result?\n",
    "\n",
    "Because the **Poisson transformation** is mathematically equivalent to the **multinomial**, *conditioned on the total count*.\n",
    "\n",
    "You model the **unconditional rates** with Poisson and then **normalize** to get conditional probabilities  thats what the multinomial does internally.\n",
    "\n",
    "The math:\n",
    "\n",
    "$$\n",
    "\\Pr(y_1, \\ldots, y_k \\mid n) = \\frac{\\prod_{i=1}^k \\mathrm{Poisson}(y_i \\mid \\lambda_i)}{\\mathrm{Poisson}\\left(n \\mid \\sum_{i=1}^k \\lambda_i\\right)}\n",
    "$$\n",
    "\n",
    "\n",
    "So you can **freely switch** between the two approaches depending on your modeling goals and data structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Link Functions in Common GLMs\n",
    "\n",
    "| **Model Type**           | **Distribution** | **Link Function**                                 | **Why This Link?**                                |\n",
    "|--------------------------|------------------|---------------------------------------------------|---------------------------------------------------|\n",
    "| Binomial (Bernoulli)     | Binomial         | $$ \\text{logit}(p) = \\log\\left(\\frac{p}{1 - p}\\right) $$ | Ensures probability $$ p \\in (0,1) $$             |\n",
    "| Poisson regression       | Poisson          | $$ \\log(\\lambda) $$                                | Ensures rate $$ \\lambda > 0 $$                    |\n",
    "| Multinomial regression   | Multinomial      | $$ \\log\\left(\\frac{p_k}{p_K}\\right) $$ (Softmax)   | Models relative log-probabilities                 |\n",
    "\n",
    "---\n",
    "\n",
    "###  1. Binomial Regression\n",
    "\n",
    "Used when outcomes are binary (e.g., success/failure).\n",
    "\n",
    "- **Distribution**:  \n",
    "  $$\n",
    "  y \\sim \\text{Binomial}(n, p)\n",
    "  $$\n",
    "\n",
    "- **Link Function**:  \n",
    "  $$\n",
    "  \\text{logit}(p) = \\log\\left(\\frac{p}{1 - p}\\right) = \\alpha + \\beta x\n",
    "  $$\n",
    "\n",
    "- **Why?**  \n",
    "  Logit turns probabilities into real numbers, which we can model with a linear equation.\n",
    "\n",
    "---\n",
    "\n",
    "###  2. Poisson Regression\n",
    "\n",
    "Used for **count data** (number of events).\n",
    "\n",
    "- **Distribution**:  \n",
    "  $$\n",
    "  y \\sim \\text{Poisson}(\\lambda)\n",
    "  $$\n",
    "\n",
    "- **Link Function**:  \n",
    "  $$\n",
    "  \\log(\\lambda) = \\alpha + \\beta x\n",
    "  $$\n",
    "\n",
    "- **Why?**  \n",
    "  The log ensures that the expected rate $$ \\lambda $$ is always positive.\n",
    "\n",
    "---\n",
    "\n",
    "###  3. Multinomial Regression\n",
    "\n",
    "Used when there are **multiple outcome categories** (e.g., red/blue/green).\n",
    "\n",
    "- **Distribution**:  \n",
    "  $$\n",
    "  y \\sim \\text{Multinomial}(p_1, ..., p_K)\n",
    "  $$\n",
    "\n",
    "- **Link Function** (Softmax / multinomial logit):  \n",
    "  $$\n",
    "  \\log\\left(\\frac{p_k}{p_K}\\right) = \\alpha_k + \\beta_k x \\quad \\text{for } k = 1, \\dots, K-1\n",
    "  $$\n",
    "\n",
    "- **Why?**  \n",
    "  The softmax ensures that all category probabilities are positive and sum to 1.\n",
    "\n",
    "---\n",
    "\n",
    "###  Summary Table\n",
    "\n",
    "| **GLM Type**     | **Outcome**        | **Link Function**                                  | **Ensures**                   |\n",
    "|------------------|--------------------|----------------------------------------------------|--------------------------------|\n",
    "| Binomial         | Binary (0/1)       | $$ \\log\\left(\\frac{p}{1 - p}\\right) $$             | $$ p \\in (0,1) $$              |\n",
    "| Poisson          | Counts (0, 1, )   | $$ \\log(\\lambda) $$                                | $$ \\lambda > 0 $$              |\n",
    "| Multinomial      | Categories         | $$ \\log\\left(\\frac{p_k}{p_K}\\right) $$ (Softmax)   | $$ \\sum p_k = 1 $$             |\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e92bb7a9aacd9667df20255a0d1807f00b256076d24753ff832859a09e84cc67"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
