{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["9E1.  Which of the following is a requirement of the simple Metropolis algorithm?\n", "\n", "(1)  The parameters must be discrete.\n", "\n", "(2)  The likelihood function must be Gaussian.\n", "\n", "(3)  The proposal distribution must be symmetric."], "attachments": {}}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "The proposal distribution must be symmetric.?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Yes, that's correct! The proposal distribution must be symmetric for the simple Metropolis algorithm.\n", "\n", "Can you explain why you think this is the case? What do you understand about how the Metropolis algorithm works that makes symmetry important?\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "because it uses gaussian dsitribution?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Not quite! The symmetry requirement isn't specifically about using a Gaussian distribution (though Gaussian distributions are symmetric).\n", "\n", "The key is in how the Metropolis algorithm calculates the acceptance probability. When the proposal distribution is symmetric, it means that the probability of proposing a move from state A to state B is the same as proposing a move from B back to A.\n", "\n", "Why do you think this symmetry property would be important for the algorithm to work correctly? What happens in the acceptance ratio calculation when the proposal distribution is symmetric?\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "it gets simple and could be used both ways?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "You're on the right track! Yes, the symmetry does make things simpler and ensures the algorithm works \"both ways.\"\n", "\n", "More specifically, when the proposal distribution is symmetric, the proposal probabilities cancel out in the acceptance ratio. This means we only need to compare the posterior probabilities (or likelihood \u00d7 prior) of the current and proposed states.\n", "\n", "Without symmetry, we'd need to account for the different proposal probabilities in both directions, which would require the more complex Metropolis-Hastings algorithm.\n", "\n", "Can you think about what this means for the detailed balance condition that MCMC methods need to satisfy?\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["9E2.\u00a0\u00a0Gibbs sampling is more efficient than the Metropolis algorithm. How does it achieve this extra efficiency? Are there any limitations to the Gibbs sampling strategy?"], "attachments": {}}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "Gibbs sampling uses conjugate priors to make smart proposals  and thus is more efficient"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "You're touching on an important aspect! Gibbs sampling does leverage conjugate relationships when they're available, but that's not the complete picture of why it's more efficient.\n", "\n", "The key efficiency gain comes from how Gibbs sampling makes proposals. Instead of proposing changes to all parameters at once (like Metropolis), what does Gibbs sampling do differently with each parameter?\n", "\n", "And what does this mean for the acceptance rate compared to Metropolis?\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "I don't know"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "No problem! Let me break this down step by step.\n", "\n", "In Metropolis, when you want to update your parameters, you propose new values for all of them at once. This proposal might get rejected.\n", "\n", "In Gibbs sampling, you do something different: you update just one parameter at a time, and you sample that parameter from its **conditional distribution** given all the other parameters.\n", "\n", "What do you think this means for whether the proposal gets accepted or rejected?\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["9E3.  Which sort of parameters can Hamiltonian Monte Carlo not handle? Can you explain why?"], "attachments": {}}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "it cant not handle discrete parameters because they don't have continuous parameter space?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Exactly right! Hamiltonian Monte Carlo cannot handle discrete parameters because it requires a continuous parameter space.\n", "\n", "Can you think about why HMC specifically needs continuous parameters? What does HMC use that requires this continuity?\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "because of the integration?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "You're getting close! HMC does involve integration, but let me help you think about this more specifically.\n", "\n", "HMC uses something called **gradients** - it needs to calculate how the log-probability changes as you move in parameter space. \n", "\n", "What mathematical operation do you need to be able to perform to calculate gradients? And why would this be impossible with discrete parameters?\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["9E4.  Explain the difference between the effective number of samples, n_eff as calculated by Stan, and the actual number of samples."], "attachments": {}}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "n_eff estimates number of uncorrelated samples and with good algorithm it could be more then real number of samples (because uncorrelated samples give more information and thus are in number more than correlated bunch of samples)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "You've got the right idea about n_eff estimating uncorrelated samples! That's the core concept.\n", "\n", "However, there's a small correction: n_eff is typically **less than** the actual number of samples, not more. This is because MCMC samples are usually correlated with each other (especially consecutive samples).\n", "\n", "Can you think about why consecutive MCMC samples would be correlated? And what this means for how much independent information we actually have?\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "they are correlated because parameters are not independent. correlated samples reduce the independent information amount we have"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "You're absolutely right! MCMC samples are correlated because each new sample depends on the previous one - that's how the Markov chain works. And yes, this correlation does reduce the amount of independent information.\n", "\n", "So if you have 1000 actual samples but they're correlated, your n_eff might be something like 200 - meaning you have the equivalent information of 200 independent samples.\n", "\n", "What do you think would make n_eff closer to the actual number of samples? In other words, what would reduce the correlation between consecutive samples?\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["9E5.  Which value should Rhat approach, when a chain is sampling the posterior distribution correctly?"], "attachments": {}}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "1.0"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Exactly right! Rhat should approach 1.0 when chains are sampling correctly.\n", "\n", "Can you explain what Rhat is actually measuring? What does it mean when Rhat is close to 1.0 versus when it's much larger than 1.0?\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["9E6.  Sketch a good trace plot for a Markov chain, one that is effectively sampling from the posterior distribution. What is good about its shape? Then sketch a trace plot for a malfunctioning Markov chain. What about its shape indicates malfunction?"], "attachments": {}}, {"cell_type": "markdown", "metadata": {}, "source": ["9E7.  Repeat the problem above, but now for a trace rank plot."], "attachments": {}}, {"cell_type": "markdown", "metadata": {}, "source": ["9E1.\u00a0\u00a0Re-estimate the terrain ruggedness model from the chapter, but now using a uniform prior for the standard deviation, sigma. The uniform prior should be dunif(0,1). Use ulam to estimate the posterior. Does the different prior have any detectible influence on the posterior distribution of sigma? Why or why not?"], "attachments": {}}], "metadata": {"kernelspec": {"display_name": "python3", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 4}